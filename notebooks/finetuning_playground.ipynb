{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "DATASET = os.getenv(\"DATASET\", \"../data/raw/enveda_library_subset_10percent.parquet\")\n",
    "BASE_MODEL = \"seyonec/ChemBERTa-zinc-base-v1\"\n",
    "MAX_FRAGMENTS = 512 # from anton, max number of mzs/intensities\n",
    "MAX_SEQ_LENGTH = 512 # base model max seq length\n",
    "SUPPLEMENTARY_DATA_DIM = 75\n",
    "ENABLE_PROFILING = False # If turned on, will profile the training\n",
    "BATCH_SIZE = int(os.getenv(\"BATCH_SIZE\", 32)) # Note: if using CUDA, it'll automatically find the optimal batch size\n",
    "NUM_EPOCHS = int(os.getenv(\"NUM_EPOCHS\", 3))\n",
    "## Set WANDB_API_KEY environment variable to enable logging to wandb\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tYhe3fk-sblS"
   },
   "source": [
    "# Messing around with ChemBERTa for fun and for education\n",
    "\n",
    "The first half of this colab is just fun experiments trying to understand ChemBERTa and it's tokenizer better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 420,
     "referenced_widgets": [
      "09447f7d805a456a8d9df123079fef9b",
      "491c2c5348da4cebb59112f76ff21867",
      "fe9ba8c3b518443ba11fd1e700dd2c24",
      "017803aafcb84942bd04ae4b6014d713",
      "7bb64ec2bc31458fac09dafbafc66030",
      "48ba3adb18ee45b085dcfcc74b8e4d19",
      "90c266cbe71d46aaade3ab541889fe07",
      "ed10bed1b375495cb383786278b88953",
      "0186504a6df04889965a532aeaa3058c",
      "9365963655bf4a6f9f65418af1213afb",
      "e8e0cfd2025e41cfb5ea102586988b28",
      "6b54217022de4316a071ee63832b77fe",
      "3cf79467e3384aeba7e957596b912e29",
      "d23dd09659b9428f8cc8083f44e04709",
      "a75c8346a8f74f43941aa43700e6b2d8",
      "e5968c80140b4a71978dbb453ee75af2",
      "285f3c7db7d448a7a9ed62b2f9324e53",
      "e02702dc96744f0bb3304606cfa29e24",
      "5810f7fa342f4543834d8274abe552f5",
      "433bfc6fb35a4199b1cedfbaf5f5a15e",
      "8d37ac015a0f4caebf5455c7c16284dc",
      "f3084e63613741098216632a5f92ef98",
      "1b99d732290a4190982895ff640ea719",
      "b284f3ac0854497fbab925eeb1ded0df",
      "01638020753146678ee0a7692603b6c2",
      "6cd3e39a24fa4ed490ea0567c51c6eee",
      "e0c994de1d654e92ac91d661932ebdfa",
      "2fa221bf24cb4bdf9695b5f593b0d818",
      "502de19d537d41a7b151ca5798f46a72",
      "b689e92305ae4b6693991b37e894f4a0",
      "50e84fe3afd94297a3f198d0dfc4654b",
      "dcc379d6713f47529ea18e30482cfa29",
      "6d6719e9bdcc411c95f60201ee37119a",
      "ec2a4d26292d4606bdcee5c2a0ea5f70",
      "cf7c0c5e780145cf90de13cd375c1cce",
      "6cc566ec625c4251908f083a4ac5e972",
      "ea2e9aa05204487fa884178986a59380",
      "6ff00156a8d1407e8a03899aede36f60",
      "57196d402589425baba759c1f92b3fee",
      "931340e0216b4cdd940a7041f14b62f1",
      "ccba3a5e8f3f4f08b79e5cad740b78b8",
      "f762dd67830c45a1b8725313b92346e4",
      "bc455b59103e4404b9f9b124d6999b3c",
      "d178a1d70c32435482c10d6824bf4131",
      "7de2b548546d4c58b4e1af83cdf618e7",
      "b5c94d24b105493796b8f70d4adb55a7",
      "be8b9e625b624ad89abb785ffdb352b9",
      "7f60ccd803d8490f990a58518b1aa4dd",
      "038ff99e2ffc4f7db7ecd54333a3d71e",
      "177f0b132f2f40b9b63a048635f0dc70",
      "47da5e41d88d4bc6bfb496adacddba30",
      "604c8bed2d6f4c04bb9ae2b78c6f4aea",
      "c83d02b54fe94d6889aae124641f43eb",
      "4df1d8407ea74a3193426b27a15eeaa9",
      "67e3bc2be1034a73be1fc0b3b97da027",
      "f53b7a64750849d18bb4df54237a1110",
      "f7b208a676664b59a90d7d38293df4dd",
      "504f51d204af44b7915060072ea359b2",
      "1f2793e435b04dc7b9502f643a98d533",
      "c9e22b771e9e49ad8354902723d9b951",
      "201c3ab1ff3a46128961e12e4edb2a01",
      "bd46618a5bce46ec840bae0adb001564",
      "ac00b65c79fc4b73a3e2d9d437001e3f",
      "d9c52550dcb8430a891ada649a35d5de",
      "4a99e037f63b4dbfa9d2f58912c46167",
      "f89668c6b88c4bad853403c95eed4fff"
     ]
    },
    "id": "KPjZH305wYrn",
    "outputId": "a36beb4d-b472-44c7-aed8-c200e1cc515c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at seyonec/ChemBERTa-zinc-base-v1 were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebacf3d3dd25488c8b44bb395853451d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/166 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b6feaed2a3e46bf9ea4e5a220e639f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/9.43k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ad6e35fd1dd46359410189a6638415f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/3.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a03d6a756cab43f195a4c230736d0dd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import (AutoModelForMaskedLM, AutoTokenizer)\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(BASE_MODEL)\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_cMXHTFeMOy"
   },
   "source": [
    "# Data prep\n",
    "\n",
    "\n",
    "These are the columns in the data set:\n",
    "\n",
    "precursor_mz - f64\n",
    "precursor_charge - f64\n",
    "mzs - list[f64]\n",
    "intensities - list[f64]\n",
    "in_silico - bool\n",
    "smiles - str\n",
    "adduct - str\n",
    "collision_energy - str\n",
    "instrument_type - str\n",
    "compound_class - str\n",
    "entropy - f64\n",
    "scaffold_smiles - str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "nyTWxIYveLsl",
    "outputId": "b83c7ecc-b236-47ff-9062-4dcb1e3db1f0"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "No such file or directory (os error 2): ../data/raw/enveda_library_subset_10percent.parquet",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mteam5\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_split\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sort_dataframe_by_scaffold, split_dataframe\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mteam5\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprepare\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensorize\n\u001b[0;32m---> 18\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATASET\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m df_sorted \u001b[38;5;241m=\u001b[39m sort_dataframe_by_scaffold(df)\n\u001b[1;32m     22\u001b[0m df_train, df_test \u001b[38;5;241m=\u001b[39m split_dataframe(df_sorted, split_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/GitHub/scratch_repository/.venv/lib/python3.11/site-packages/polars/_utils/deprecation.py:92\u001b[0m, in \u001b[0;36mdeprecate_renamed_parameter.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(function)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m     89\u001b[0m     _rename_keyword_argument(\n\u001b[1;32m     90\u001b[0m         old_name, new_name, kwargs, function\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m, version\n\u001b[1;32m     91\u001b[0m     )\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/scratch_repository/.venv/lib/python3.11/site-packages/polars/_utils/deprecation.py:92\u001b[0m, in \u001b[0;36mdeprecate_renamed_parameter.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(function)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m     89\u001b[0m     _rename_keyword_argument(\n\u001b[1;32m     90\u001b[0m         old_name, new_name, kwargs, function\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m, version\n\u001b[1;32m     91\u001b[0m     )\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/scratch_repository/.venv/lib/python3.11/site-packages/polars/io/parquet/functions.py:241\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(source, columns, n_rows, row_index_name, row_index_offset, parallel, use_statistics, hive_partitioning, glob, schema, hive_schema, try_parse_hive_dates, rechunk, low_memory, storage_options, retries, use_pyarrow, pyarrow_options, memory_map, include_file_paths, allow_missing_columns)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    239\u001b[0m         lf \u001b[38;5;241m=\u001b[39m lf\u001b[38;5;241m.\u001b[39mselect(columns)\n\u001b[0;32m--> 241\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/scratch_repository/.venv/lib/python3.11/site-packages/polars/lazyframe/frame.py:2050\u001b[0m, in \u001b[0;36mLazyFrame.collect\u001b[0;34m(self, type_coercion, predicate_pushdown, projection_pushdown, simplify_expression, slice_pushdown, comm_subplan_elim, comm_subexpr_elim, cluster_with_columns, collapse_joins, no_optimization, streaming, engine, background, _eager, **_kwargs)\u001b[0m\n\u001b[1;32m   2048\u001b[0m \u001b[38;5;66;03m# Only for testing purposes\u001b[39;00m\n\u001b[1;32m   2049\u001b[0m callback \u001b[38;5;241m=\u001b[39m _kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost_opt_callback\u001b[39m\u001b[38;5;124m\"\u001b[39m, callback)\n\u001b[0;32m-> 2050\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap_df(\u001b[43mldf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: No such file or directory (os error 2): ../data/raw/enveda_library_subset_10percent.parquet"
     ]
    }
   ],
   "source": [
    "# import the data (with pandas?)\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "## Load the dataset (for some reason this didn't work for me)\n",
    "# df = pd.read_parquet('enveda_library_subset 2.parquet')\n",
    "\n",
    "# print(df.head())\n",
    "\n",
    "\n",
    "# custom Dataset class for all the types of data.\n",
    "# I think we might want to make a new 'column' of data that combines mzs and intensities into \"label\"\n",
    "\n",
    "from src.team5.data.data_loader import SMILESDataset\n",
    "from src.team5.data.data_split import sort_dataframe_by_scaffold, split_dataframe\n",
    "from src.team5.data.prepare import tensorize\n",
    "\n",
    "df = pl.read_parquet(DATASET)\n",
    "\n",
    "df_sorted = sort_dataframe_by_scaffold(df)\n",
    "\n",
    "df_train, df_test = split_dataframe(df_sorted, split_ratio=0.9)\n",
    "\n",
    "# Print column names\n",
    "print(df_train.columns)\n",
    "print(df_test.columns)\n",
    "print(df_train.head())\n",
    "print(df_test.head())\n",
    "\n",
    "(train_tokenized_smiles, train_attention_mask, train_labels, train_supplementary_data) = tensorize(df_train, split=\"train\")\n",
    "(test_tokenized_smiles, test_attention_mask, test_labels, test_supplementary_data) = tensorize(df_test, split=\"test\")\n",
    "\n",
    "train_dataset = SMILESDataset(train_tokenized_smiles, train_attention_mask, train_labels, train_supplementary_data)\n",
    "test_dataset = SMILESDataset(test_tokenized_smiles, test_attention_mask, test_labels, test_supplementary_data)\n",
    "\n",
    "## batch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': torch.int64, 'attention_mask': torch.int64, 'labels': torch.float32, 'supplementary_data': torch.float32}\n"
     ]
    }
   ],
   "source": [
    "print({k:v.dtype for k,v in train_dataset[0].items()})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fENovYIHeaOO"
   },
   "source": [
    "# Custom model for our problem\n",
    "This is probably the most important part in terms of design choices. We are changing the ChemBERTa model by adding on something at the end. This new module will take the hidden SMILES embedding from the last hidden layer as input. It will also take in all the other data about the precusor molecule and experimental conditions (eg, precusor mz, collison energy etc). For now, let's call that supplementary data.\n",
    "\n",
    "I've written the simplest possible thing here: a single linear layer that takes the embedding of the entire seq, concatinated with all the supplementary data for the example. It outputs \"labels\", which is mzs and intensities zipped together.\n",
    "\n",
    "The reason for making a single module output both mzs and intensities is because there needs to be the same number of fragments per example, and the two numbers are very related.\n",
    "\n",
    "A single linear layer is probably a terrible choice though, since this is the only layer that sees all the supplementary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "rMogXqZteZ1Z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomChemBERTaModel(\n",
      "  (model): RobertaForMaskedLM(\n",
      "    (roberta): RobertaModel(\n",
      "      (embeddings): RobertaEmbeddings(\n",
      "        (word_embeddings): Embedding(767, 768, padding_idx=1)\n",
      "        (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "        (token_type_embeddings): Embedding(1, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): RobertaEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0-5): 6 x RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSdpaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (lm_head): RobertaLMHead(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (decoder): Linear(in_features=768, out_features=767, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (final_layers): FinalLayers(\n",
      "    (layer1): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (activation1): ReLU()\n",
      "    (layer2): Linear(in_features=843, out_features=16, bias=True)\n",
      "    (layer3): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "  )\n",
      ")\n",
      "model.roberta.embeddings.word_embeddings.weight has shape torch.Size([767, 768])\n",
      "model.roberta.embeddings.position_embeddings.weight has shape torch.Size([514, 768])\n",
      "model.roberta.embeddings.token_type_embeddings.weight has shape torch.Size([1, 768])\n",
      "model.roberta.embeddings.LayerNorm.weight has shape torch.Size([768])\n",
      "model.roberta.embeddings.LayerNorm.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.0.attention.self.query.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.0.attention.self.query.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.0.attention.self.key.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.0.attention.self.key.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.0.attention.self.value.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.0.attention.self.value.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.0.attention.output.dense.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.0.attention.output.dense.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.0.attention.output.LayerNorm.weight has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.0.attention.output.LayerNorm.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.0.intermediate.dense.weight has shape torch.Size([3072, 768])\n",
      "model.roberta.encoder.layer.0.intermediate.dense.bias has shape torch.Size([3072])\n",
      "model.roberta.encoder.layer.0.output.dense.weight has shape torch.Size([768, 3072])\n",
      "model.roberta.encoder.layer.0.output.dense.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.0.output.LayerNorm.weight has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.0.output.LayerNorm.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.1.attention.self.query.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.1.attention.self.query.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.1.attention.self.key.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.1.attention.self.key.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.1.attention.self.value.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.1.attention.self.value.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.1.attention.output.dense.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.1.attention.output.dense.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.1.attention.output.LayerNorm.weight has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.1.attention.output.LayerNorm.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.1.intermediate.dense.weight has shape torch.Size([3072, 768])\n",
      "model.roberta.encoder.layer.1.intermediate.dense.bias has shape torch.Size([3072])\n",
      "model.roberta.encoder.layer.1.output.dense.weight has shape torch.Size([768, 3072])\n",
      "model.roberta.encoder.layer.1.output.dense.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.1.output.LayerNorm.weight has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.1.output.LayerNorm.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.2.attention.self.query.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.2.attention.self.query.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.2.attention.self.key.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.2.attention.self.key.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.2.attention.self.value.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.2.attention.self.value.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.2.attention.output.dense.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.2.attention.output.dense.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.2.attention.output.LayerNorm.weight has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.2.attention.output.LayerNorm.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.2.intermediate.dense.weight has shape torch.Size([3072, 768])\n",
      "model.roberta.encoder.layer.2.intermediate.dense.bias has shape torch.Size([3072])\n",
      "model.roberta.encoder.layer.2.output.dense.weight has shape torch.Size([768, 3072])\n",
      "model.roberta.encoder.layer.2.output.dense.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.2.output.LayerNorm.weight has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.2.output.LayerNorm.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.3.attention.self.query.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.3.attention.self.query.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.3.attention.self.key.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.3.attention.self.key.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.3.attention.self.value.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.3.attention.self.value.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.3.attention.output.dense.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.3.attention.output.dense.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.3.attention.output.LayerNorm.weight has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.3.attention.output.LayerNorm.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.3.intermediate.dense.weight has shape torch.Size([3072, 768])\n",
      "model.roberta.encoder.layer.3.intermediate.dense.bias has shape torch.Size([3072])\n",
      "model.roberta.encoder.layer.3.output.dense.weight has shape torch.Size([768, 3072])\n",
      "model.roberta.encoder.layer.3.output.dense.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.3.output.LayerNorm.weight has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.3.output.LayerNorm.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.4.attention.self.query.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.4.attention.self.query.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.4.attention.self.key.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.4.attention.self.key.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.4.attention.self.value.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.4.attention.self.value.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.4.attention.output.dense.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.4.attention.output.dense.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.4.attention.output.LayerNorm.weight has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.4.attention.output.LayerNorm.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.4.intermediate.dense.weight has shape torch.Size([3072, 768])\n",
      "model.roberta.encoder.layer.4.intermediate.dense.bias has shape torch.Size([3072])\n",
      "model.roberta.encoder.layer.4.output.dense.weight has shape torch.Size([768, 3072])\n",
      "model.roberta.encoder.layer.4.output.dense.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.4.output.LayerNorm.weight has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.4.output.LayerNorm.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.5.attention.self.query.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.5.attention.self.query.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.5.attention.self.key.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.5.attention.self.key.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.5.attention.self.value.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.5.attention.self.value.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.5.attention.output.dense.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.5.attention.output.dense.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.5.attention.output.LayerNorm.weight has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.5.attention.output.LayerNorm.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.5.intermediate.dense.weight has shape torch.Size([3072, 768])\n",
      "model.roberta.encoder.layer.5.intermediate.dense.bias has shape torch.Size([3072])\n",
      "model.roberta.encoder.layer.5.output.dense.weight has shape torch.Size([768, 3072])\n",
      "model.roberta.encoder.layer.5.output.dense.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.5.output.LayerNorm.weight has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.5.output.LayerNorm.bias has shape torch.Size([768])\n",
      "model.lm_head.bias has shape torch.Size([767])\n",
      "model.lm_head.dense.weight has shape torch.Size([768, 768])\n",
      "model.lm_head.dense.bias has shape torch.Size([768])\n",
      "model.lm_head.layer_norm.weight has shape torch.Size([768])\n",
      "model.lm_head.layer_norm.bias has shape torch.Size([768])\n",
      "final_layers.layer1.weight has shape torch.Size([128, 512])\n",
      "final_layers.layer1.bias has shape torch.Size([128])\n",
      "final_layers.layer2.weight has shape torch.Size([16, 843])\n",
      "final_layers.layer2.bias has shape torch.Size([16])\n",
      "final_layers.layer3.weight has shape torch.Size([1024, 2048])\n",
      "final_layers.layer3.bias has shape torch.Size([1024])\n"
     ]
    }
   ],
   "source": [
    "from src.team5.models.custom_model import CustomChemBERTaModel\n",
    "\n",
    "MS_model = CustomChemBERTaModel(model, MAX_FRAGMENTS, MAX_SEQ_LENGTH, SUPPLEMENTARY_DATA_DIM)\n",
    "\n",
    "print(MS_model)\n",
    "\n",
    "for name, param in MS_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name} has shape {param.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IDwMMYCaYAZK"
   },
   "source": [
    "# LoRA config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "y0PF6-qGZ683"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,324,800 || all params: 48,608,383 || trainable%: 4.7827\n",
      "base_model.model.model.roberta.encoder.layer.0.attention.self.query.lora_A.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.0.attention.self.query.lora_B.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.0.attention.self.value.lora_A.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.0.attention.self.value.lora_B.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.1.attention.self.query.lora_A.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.1.attention.self.query.lora_B.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.1.attention.self.value.lora_A.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.1.attention.self.value.lora_B.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.2.attention.self.query.lora_A.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.2.attention.self.query.lora_B.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.2.attention.self.value.lora_A.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.2.attention.self.value.lora_B.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.3.attention.self.query.lora_A.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.3.attention.self.query.lora_B.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.3.attention.self.value.lora_A.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.3.attention.self.value.lora_B.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.4.attention.self.query.lora_A.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.4.attention.self.query.lora_B.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.4.attention.self.value.lora_A.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.4.attention.self.value.lora_B.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.5.attention.self.query.lora_A.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.5.attention.self.query.lora_B.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.5.attention.self.value.lora_A.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.5.attention.self.value.lora_B.default.weight is trainable\n",
      "base_model.model.final_layers.modules_to_save.default.layer1.weight is trainable\n",
      "base_model.model.final_layers.modules_to_save.default.layer1.bias is trainable\n",
      "base_model.model.final_layers.modules_to_save.default.layer2.weight is trainable\n",
      "base_model.model.final_layers.modules_to_save.default.layer2.bias is trainable\n",
      "base_model.model.final_layers.modules_to_save.default.layer3.weight is trainable\n",
      "base_model.model.final_layers.modules_to_save.default.layer3.bias is trainable\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"query\", \"value\"],\n",
    "    modules_to_save=[\n",
    "        \"final_layers\"\n",
    "    ],  # change this to the name of the new modules at the end.\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(MS_model, peft_config)\n",
    "\n",
    "peft_model.print_trainable_parameters()  # check that it's training the right things\n",
    "for name, param in peft_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name} is trainable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-q-zpo0kaF0p"
   },
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import torch.nn as nn\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "class ProfilingCallback(TrainerCallback):\n",
    "    def __init__(self, device, n_steps=10):\n",
    "        self.device = device\n",
    "        self.n_steps = n_steps\n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if state.global_step % self.n_steps == 0:\n",
    "            with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "                         profile_memory=True, record_shapes=True) as prof:\n",
    "                with record_function(\"model_inference\"):\n",
    "                    # Run a forward pass\n",
    "                    example = train_dataset[0]\n",
    "                    # Each field in the example is a tensor, so we need to add a batch dimension to the front of each\n",
    "                    example = {k: v.unsqueeze(0).to(self.device) for k, v in example.items()}\n",
    "                    peft_model(**example)\n",
    "            \n",
    "            print(f\"Step {state.global_step}\")\n",
    "            print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=20))\n",
    "            print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=20))\n",
    "            print(prof.key_averages().table(sort_by=\"cpu_memory_usage\", row_limit=20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 8.868408203125 MB\n"
     ]
    }
   ],
   "source": [
    "# Print how much memory the model will need on GPU to train\n",
    "print(f\"Model size: {sum(p.numel() for p in peft_model.parameters() if p.requires_grad) * 4 / (1024 ** 2)} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lars/workspace/scratch_repository/.venv/lib/python3.12/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/Users/lars/workspace/scratch_repository/.venv/lib/python3.12/site-packages/transformers/training_args.py:2199: UserWarning: `use_mps_device` is deprecated and will be removed in version 5.0 of 🤗 Transformers. `mps` device will be used by default if available similar to the way `cuda` device is used.Therefore, no action from user is required. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66498a6b1a1b42248d2c5da9f97778ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8766 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datetime import date, datetime\n",
    "import os\n",
    "from transformers import TrainingArguments\n",
    "import torch\n",
    "from transformers import Trainer\n",
    "\n",
    "device_type = \"cpu\"\n",
    "if torch.backends.mps.is_available():\n",
    "    device_type = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    device_type = \"cuda\"\n",
    "print(f\"Using device: {device_type}\")\n",
    "device = torch.device(device_type)\n",
    "peft_model.to(device)\n",
    "for param in peft_model.parameters():\n",
    "    param.data = param.data.to(device)\n",
    "\n",
    "if ENABLE_PROFILING:\n",
    "    # Print where each tensor is placed\n",
    "    for name, param in peft_model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f\"{name} is placed on {param.device}\")\n",
    "\n",
    "# Enable logging to wandb if WANDB_API_KEY is set\n",
    "wandb_enabled = os.getenv(\"WANDB_API_KEY\") is not None\n",
    "wandb_api_key = os.getenv(\"WANDB_API_KEY\", None)\n",
    "os.environ[\"WANDB_PROJECT\"] = \"hackathon\"\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"end\"\n",
    "os.environ[\"WANDB_WATCH\"] = \"false\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"../logs/training_{date.today().strftime('%Y-%m-%d')}-{datetime.now().strftime('%H-%M-%S')}\",\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    dataloader_num_workers=8,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=0.1,\n",
    "    label_names=[\"labels\"],\n",
    "    report_to=\"wandb\" if wandb_enabled else \"none\",\n",
    "    auto_find_batch_size=(device_type == \"cuda\"),\n",
    "    use_mps_device=(device_type == \"mps\"),\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    ")\n",
    "\n",
    "callbacks = []\n",
    "if ENABLE_PROFILING:\n",
    "    callbacks.append(ProfilingCallback(device, n_steps=10))\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2D8bfVV9keeM"
   },
   "source": [
    "# Choices that affect the whole architecture\n",
    "\n",
    "*   Format for the supplementary data\n",
    "*   Format for the label data\n",
    "*   The format of the output of the new model\n",
    "\n",
    "\n",
    "\n",
    "### More modular choices (that are important)\n",
    "\n",
    "\n",
    "*   Whether we have to predict compound_class at inference\n",
    "*   Include in_silico data?\n",
    "*   Architeture of the modified ChemBERTa model\n",
    "*   LoRA parameters\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
