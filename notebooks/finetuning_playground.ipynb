{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gx11hYWe4hjx",
    "outputId": "7f0c1279-d448-4898-f652-0e9da18ba524"
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install simpletransformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tYhe3fk-sblS"
   },
   "source": [
    "# Messing around with ChemBERTa for fun and for education\n",
    "\n",
    "The first half of this colab is just fun experiments trying to understand ChemBERTa and it's tokenizer better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 420,
     "referenced_widgets": [
      "09447f7d805a456a8d9df123079fef9b",
      "491c2c5348da4cebb59112f76ff21867",
      "fe9ba8c3b518443ba11fd1e700dd2c24",
      "017803aafcb84942bd04ae4b6014d713",
      "7bb64ec2bc31458fac09dafbafc66030",
      "48ba3adb18ee45b085dcfcc74b8e4d19",
      "90c266cbe71d46aaade3ab541889fe07",
      "ed10bed1b375495cb383786278b88953",
      "0186504a6df04889965a532aeaa3058c",
      "9365963655bf4a6f9f65418af1213afb",
      "e8e0cfd2025e41cfb5ea102586988b28",
      "6b54217022de4316a071ee63832b77fe",
      "3cf79467e3384aeba7e957596b912e29",
      "d23dd09659b9428f8cc8083f44e04709",
      "a75c8346a8f74f43941aa43700e6b2d8",
      "e5968c80140b4a71978dbb453ee75af2",
      "285f3c7db7d448a7a9ed62b2f9324e53",
      "e02702dc96744f0bb3304606cfa29e24",
      "5810f7fa342f4543834d8274abe552f5",
      "433bfc6fb35a4199b1cedfbaf5f5a15e",
      "8d37ac015a0f4caebf5455c7c16284dc",
      "f3084e63613741098216632a5f92ef98",
      "1b99d732290a4190982895ff640ea719",
      "b284f3ac0854497fbab925eeb1ded0df",
      "01638020753146678ee0a7692603b6c2",
      "6cd3e39a24fa4ed490ea0567c51c6eee",
      "e0c994de1d654e92ac91d661932ebdfa",
      "2fa221bf24cb4bdf9695b5f593b0d818",
      "502de19d537d41a7b151ca5798f46a72",
      "b689e92305ae4b6693991b37e894f4a0",
      "50e84fe3afd94297a3f198d0dfc4654b",
      "dcc379d6713f47529ea18e30482cfa29",
      "6d6719e9bdcc411c95f60201ee37119a",
      "ec2a4d26292d4606bdcee5c2a0ea5f70",
      "cf7c0c5e780145cf90de13cd375c1cce",
      "6cc566ec625c4251908f083a4ac5e972",
      "ea2e9aa05204487fa884178986a59380",
      "6ff00156a8d1407e8a03899aede36f60",
      "57196d402589425baba759c1f92b3fee",
      "931340e0216b4cdd940a7041f14b62f1",
      "ccba3a5e8f3f4f08b79e5cad740b78b8",
      "f762dd67830c45a1b8725313b92346e4",
      "bc455b59103e4404b9f9b124d6999b3c",
      "d178a1d70c32435482c10d6824bf4131",
      "7de2b548546d4c58b4e1af83cdf618e7",
      "b5c94d24b105493796b8f70d4adb55a7",
      "be8b9e625b624ad89abb785ffdb352b9",
      "7f60ccd803d8490f990a58518b1aa4dd",
      "038ff99e2ffc4f7db7ecd54333a3d71e",
      "177f0b132f2f40b9b63a048635f0dc70",
      "47da5e41d88d4bc6bfb496adacddba30",
      "604c8bed2d6f4c04bb9ae2b78c6f4aea",
      "c83d02b54fe94d6889aae124641f43eb",
      "4df1d8407ea74a3193426b27a15eeaa9",
      "67e3bc2be1034a73be1fc0b3b97da027",
      "f53b7a64750849d18bb4df54237a1110",
      "f7b208a676664b59a90d7d38293df4dd",
      "504f51d204af44b7915060072ea359b2",
      "1f2793e435b04dc7b9502f643a98d533",
      "c9e22b771e9e49ad8354902723d9b951",
      "201c3ab1ff3a46128961e12e4edb2a01",
      "bd46618a5bce46ec840bae0adb001564",
      "ac00b65c79fc4b73a3e2d9d437001e3f",
      "d9c52550dcb8430a891ada649a35d5de",
      "4a99e037f63b4dbfa9d2f58912c46167",
      "f89668c6b88c4bad853403c95eed4fff"
     ]
    },
    "id": "KPjZH305wYrn",
    "outputId": "a36beb4d-b472-44c7-aed8-c200e1cc515c"
   },
   "outputs": [],
   "source": [
    "from transformers import (AutoModelForMaskedLM, AutoTokenizer, RobertaModel,\n",
    "                          RobertaTokenizer, pipeline)\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"seyonec/PubChem10M_SMILES_BPE_450k\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"seyonec/PubChem10M_SMILES_BPE_450k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c_UcbETKtrin"
   },
   "source": [
    "## Let's print the tokenizer and model's stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GqXGRKf_lgF1",
    "outputId": "b14003c5-a926-4d7a-dc5f-be709e2c38cb"
   },
   "outputs": [],
   "source": [
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tk8oJJtMm-cJ",
    "outputId": "110c15bb-7f43-4b94-8b30-beba865b5d67"
   },
   "outputs": [],
   "source": [
    "print(model.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fwxVQOOGxcEX"
   },
   "source": [
    "One bizarre thing to note here: The default tokenizer has a smaller vocab_size than the vocab_size of the model. What's going on? Not sure, but I think it's because the vocab_size of the model is based off usual language, where as the tokenizer is much smaller since it only needs to cover all possible SMILES. It's weird though, that they didn't adjust the vocab_size in the model. I guess that the weights for last 52000-7924 vocabs just don't count. Seems super wasteful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dDxBKYAX1dDJ"
   },
   "source": [
    "### Names of all the weights in ChemBERTa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2zvzY6ZL3OIM",
    "outputId": "9678e0f4-3953-4eeb-d059-70d91d06609e"
   },
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name} has shape {param.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k13OI3jEs6-5"
   },
   "source": [
    "## Default inference pipeline in ChemBERTa\n",
    "\n",
    "ChemBERTa is an LLM trained on the masked token task. Let's see how well it does it with an example. Below is an example of a masked smiles and what it \"should\" be. The fill-mask pipeline gives ChemBERTa's top 5 guesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Oei2EU0DwZjU",
    "outputId": "f8ae35c0-40ab-41a5-8f78-773b59dd1f47"
   },
   "outputs": [],
   "source": [
    "fill_mask = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "smiles_mask = \"C1=CC=CC<mask>C1\"\n",
    "smiles = \"C1=CC=CC=C1\"\n",
    "\n",
    "masked_smi = fill_mask(smiles_mask)\n",
    "\n",
    "for smi in masked_smi:\n",
    "    print(smi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejI2RJeGtf2t"
   },
   "source": [
    "Pretty good! It gave a probability of 97% to the correct SMILES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7S28IjFiuA7M"
   },
   "source": [
    "## How to interpret logits for other tokens?\n",
    "\n",
    "When a token isn't masked, what should you make of the logits/\"probability\" for it? For each token there a logit/prob for each of the possible 52000 tokens that could have been there. One way to intepret it is as if that token was actually masked, and the model is giving probabilities for all the tokens that could potentially go there. Another way to think of it is that it's about how \"natural\" this token is in this spot. If the model gives the token a high score for itself, it expected to see it there. Since we are giving the model a very natural sequence, we'd expect that it's scores for each of the tokens is highest for the actual token. Let's see.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FmBxqR13wfyl",
    "outputId": "ed47ce6b-652f-4e49-a012-0a919bb4cabb"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "sequence = f\"C1=CC=CC=CC1\"\n",
    "input = tokenizer.encode(sequence, return_tensors=\"pt\")\n",
    "\n",
    "for i in range(len(input[0])):\n",
    "    decode = tokenizer.decode(input[0][i])\n",
    "    encode = input[0][i]\n",
    "    print(f\"{decode} is the token {encode}\")\n",
    "\n",
    "token_logits = model(input)[0]\n",
    "print(f\"token logits shape {token_logits.shape}\")\n",
    "\n",
    "for i, token_id in enumerate(input[0]):\n",
    "    print(f\"token_id is {token_id}\")\n",
    "    # Get the logits for the i-th position\n",
    "    logits_for_token_position = token_logits[\n",
    "        0, i, :\n",
    "    ]  # Shape: [52000], all logits for this position\n",
    "    probability_for_token_position = torch.softmax(logits_for_token_position, dim=0)\n",
    "\n",
    "    logit_for_correct_token = logits_for_token_position[token_id]\n",
    "    prob_for_correct_token = probability_for_token_position[token_id]\n",
    "    print(\n",
    "        f\"The probability of the correct token is {round(prob_for_correct_token.item(),2)}\"\n",
    "    )\n",
    "\n",
    "    # Find the maximum logit value at this position\n",
    "    max_logit = logits_for_token_position.max().item()\n",
    "\n",
    "    # Check if the logit for the actual token is the highest\n",
    "    if logit_for_correct_token == max_logit:\n",
    "        print(f\"Position {i}: Correct: {token_id} has the highest logit.\")\n",
    "    else:\n",
    "        print(f\"Position {i}: Incorrect: {token_id} does NOT have the highest logit.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GYVaGoYBw9Vt"
   },
   "source": [
    "The only two it got incorrect where the start and end, which is fair enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FsAFLnGWfFWy"
   },
   "source": [
    "## Testing it out on masked sequences again- this time without the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sifbg2_ifEhZ",
    "outputId": "17c33a52-bd46-4cf1-a5bf-0fee5f3aa7be"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "sequence = f\"C1=CC=CC={tokenizer.mask_token}1\"\n",
    "input = tokenizer.encode(sequence, return_tensors=\"pt\")\n",
    "mask_token_index = torch.where(input == tokenizer.mask_token_id)[1]\n",
    "\n",
    "mask_token_probs = F.softmax(token_logits[0, mask_token_index, :][0], dim=0)\n",
    "top_5_tokens = torch.topk(mask_token_probs, 5).indices.tolist()\n",
    "\n",
    "for token in top_5_tokens:\n",
    "    smi = sequence.replace(tokenizer.mask_token, tokenizer.decode([token]))\n",
    "    print(f\"{smi} has a probability of {round(mask_token_probs[token].item(),2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WhSi8hQzefb1"
   },
   "source": [
    "## Embeddings from ChemBERTa\n",
    "\n",
    "Each token in a SMILES sequence is embedded as a vector of dimension hidden_dimension. Then the model updates this embedding at every layer. In the last layer (before the big projection matrix back to vocab_size) the embedding is hopefully very rich and meaningful. The transformer model should have encoded important relationships between parts of the molecule into these vectors. That's why we'd like to be able to access these and use them in later parts of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gk371_rbdteS",
    "outputId": "dffddfc5-8942-4159-dfc4-8b5ab2351a4c"
   },
   "outputs": [],
   "source": [
    "sequence = f\"C1=CC=CC=CC1\"\n",
    "inputs = tokenizer(sequence, return_tensors=\"pt\")\n",
    "outputs = model(**inputs, output_hidden_states=True)\n",
    "hidden_states = outputs.hidden_states\n",
    "print(\n",
    "    f\"The number of hidden states is {len(hidden_states)}, because there's one after each layer (except the last)\"\n",
    ")\n",
    "print(f\"The shape of the hidden states is {hidden_states[0].shape}\")\n",
    "number_of_tokens = hidden_states[0].shape[1]\n",
    "hidden_dimension = hidden_states[0].shape[2]\n",
    "\n",
    "# Let's look at the embeddings in the 0th hidden state versus in the final\n",
    "original_token_embeddings = hidden_states[0][0]\n",
    "last_token_embeddings = hidden_states[-1][0]\n",
    "\n",
    "for i in range(number_of_tokens):\n",
    "    dot_product = torch.dot(original_token_embeddings[i], last_token_embeddings[i])\n",
    "    print(\n",
    "        f\"The {i}th token has dot product between original and final of {round(dot_product.item(),2)}\"\n",
    "    )\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"On the other hand, this is what the dot product of two random vectors looks like\"\n",
    ")\n",
    "dot_product_count = 0\n",
    "for i in range(1000):\n",
    "    random_vector1 = torch.randn(hidden_dimension)\n",
    "    random_vector2 = torch.randn(hidden_dimension)\n",
    "    dot_product_count += torch.abs(torch.dot(random_vector1, random_vector2)).item()\n",
    "\n",
    "print(\n",
    "    f\"The average absolute value of the dot product of 100 random vectors is {round(dot_product_count/1000,2)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_cMXHTFeMOy"
   },
   "source": [
    "# Data prep\n",
    "\n",
    "\n",
    "These are the columns in the data set:\n",
    "\n",
    "precursor_mz - f64\n",
    "precursor_charge - f64\n",
    "mzs - list[f64]\n",
    "intensities - list[f64]\n",
    "in_silico - bool\n",
    "smiles - str\n",
    "adduct - str\n",
    "collision_energy - str\n",
    "instrument_type - str\n",
    "compound_class - str\n",
    "entropy - f64\n",
    "scaffold_smiles - str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "nyTWxIYveLsl",
    "outputId": "b83c7ecc-b236-47ff-9062-4dcb1e3db1f0"
   },
   "outputs": [],
   "source": [
    "# import the data (with pandas?)\n",
    "import pandas as pd\n",
    "\n",
    "## Load the dataset (for some reason this didn't work for me)\n",
    "# df = pd.read_parquet('enveda_library_subset 2.parquet')\n",
    "\n",
    "# print(df.head())\n",
    "\n",
    "\n",
    "# tokenize the SMILES. Do we need to pad? If so, what's the max length\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"smiles\"], truncation=True, padding=\"max_length\", max_length=128\n",
    "    )\n",
    "\n",
    "\n",
    "# custom Dataset class for all the types of data.\n",
    "# I think we might want to make a new 'column' of data that combines mzs and intensities into \"label\"\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class SMILESDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.smiles = dataframe[\"smiles\"].tolist()\n",
    "        self.precursor_mz = dataframe[\"precursor_mz\"].tolist()\n",
    "        self.precursor_charge = dataframe[\"precursor_charge\"].tolist()\n",
    "        self.collision_energy = dataframe[\"collision_energy\"].tolist()\n",
    "        self.instrument_type = dataframe[\"instrument_type\"].tolist()\n",
    "        self.in_silico_label = dataframe[\"in_silico_label\"].tolist()\n",
    "        self.adduct = dataframe[\"adduct\"].tolist()\n",
    "        self.compound_class = dataframe[\"compound_class\"].tolist()\n",
    "        self.mzs = dataframe[\"mzs\"].tolist()\n",
    "        self.intensities = dataframe[\"intensities\"].tolist()\n",
    "\n",
    "        # Create labels as a 2D array of mzs and intensities put together. Or have it flat and just concat both\n",
    "        # self.labels = #TODO\n",
    "\n",
    "        # Create supplementary data as a long concatinated list of all the supplementary data\n",
    "        # self.supplementary_data = #TODO\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.smiles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        smiles = self.smiles[idx]\n",
    "        precursor_mz = self.precursor_mz[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Tokenize SMILES\n",
    "        inputs = self.tokenizer(\n",
    "            smiles,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=128,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # Prepare item\n",
    "        item = {\n",
    "            key: val.squeeze(0) for key, val in inputs.items()\n",
    "        }  # Remove batch dimension\n",
    "        item[\"precursor_mz\"] = torch.tensor(precursor_mz, dtype=torch.float)\n",
    "        item[\"labels\"] = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return item\n",
    "\n",
    "\n",
    "# test/train split\n",
    "# Use Murcko scaffold and spectral entropy splitting for this, rather than random.\n",
    "# This will ensure that similar molecules don't go into both training and test,\n",
    "# causing cross contamination and over fitting.\n",
    "\n",
    "\n",
    "def split_data(df):\n",
    "    # implement something not random here\n",
    "    return train_test_split(df, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "# train_df, eval_df = split_data(df)\n",
    "\n",
    "# train_dataset = SMILESDataset(train_df, tokenizer)\n",
    "# eval_dataset = SMILESDataset(eval_df, tokenizer)\n",
    "\n",
    "## batch?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fENovYIHeaOO"
   },
   "source": [
    "# Custom model for our problem\n",
    "This is probably the most important part in terms of design choices. We are changing the ChemBERTa model by adding on something at the end. This new module will take the hidden SMILES embedding from the last hidden layer as input. It will also take in all the other data about the precusor molecule and experimental conditions (eg, precusor mz, collison energy etc). For now, let's call that supplementary data.\n",
    "\n",
    "I've written the simplest possible thing here: a single linear layer that takes the embedding of the entire seq, concatinated with all the supplementary data for the example. It outputs \"labels\", which is mzs and intensities zipped together.\n",
    "\n",
    "The reason for making a single module output both mzs and intensities is because there needs to be the same number of fragments per example, and the two numbers are very related.\n",
    "\n",
    "A single linear layer is probably a terrible choice though, since this is the only layer that sees all the supplementary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rMogXqZteZ1Z"
   },
   "outputs": [],
   "source": [
    "import einops\n",
    "\n",
    "max_fragments = 10  # find out what the maximum number of fragments is in the data\n",
    "\n",
    "\n",
    "class CustomChemBERTaModel(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(CustomChemBERTaModel, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "        # Get hidden size from the ChemBERTa model configuration\n",
    "        hidden_size = self.model.config.hidden_size\n",
    "        seq_length = 128  # Should we set a max sequence length and pad like this?\n",
    "\n",
    "        # Define a linear layer to output 2 numbers (mz and intensity) per fragment\n",
    "        self.linear = nn.Linear(hidden_size * seq_length, 2 * max_fragments)\n",
    "\n",
    "    def forward(self, input_ids, supplementary_data=None, labels=None):\n",
    "        # Pass inputs through ChemBERTa\n",
    "        outputs = self.model(input_ids=input_ids, output_hidden_states=True)\n",
    "\n",
    "        # Extract last hidden state (embeddings)\n",
    "        last_hidden_state = outputs.hidden_state[-1]\n",
    "        flatten_hidden_state = einops.rearrange(last_hidden_state, \"b s h -> b (s h)\")\n",
    "\n",
    "        # Pass in supplementary data and then contat it with flatten_hidden_state\n",
    "        # state = contat(flatten_hidden_state, supplementary_data) #TODO\n",
    "\n",
    "        # Pass through the linear layer\n",
    "        predicted_output_flat = self.linear(\n",
    "            state\n",
    "        )  # Shape: [batch_size, 2 * max_fragments]\n",
    "        predicted_output = einops.rearrange(\n",
    "            predicted_output_flat, \"b (h f) -> b h f\", f=max_fragments\n",
    "        )\n",
    "\n",
    "        # calculate the loss by comparing to labels\n",
    "        loss = 0  # TODO use the loss function they mentioned on discord (I think dot product similarity)\n",
    "\n",
    "        return predicted_output, loss\n",
    "\n",
    "\n",
    "# MS_model = CustomChemBERTaModel(model, supplementary_data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IDwMMYCaYAZK"
   },
   "source": [
    "# LoRA config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y0PF6-qGZ683"
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"key\", \"query\", \"value\"] # they seem to drop off the \"key\" often?\n",
    "    modules_to_save=[\"classifier\"] # change this to the name of the new modules at the end.\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "\n",
    "# I don't think this is stickly necessary?\n",
    "# In fact, may even be bad since it might freeze params in our last layer:\n",
    "for param in peft_model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters() #check that it's training the right things"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-q-zpo0kaF0p"
   },
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I3YvQNVqcGyF"
   },
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=16, shuffle=True, collate_fn=data_collator\n",
    ")\n",
    "optimizer = AdamW(peft_model.parameters(), lr=5e-5)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "peft_model.to(device)\n",
    "peft_model.train()\n",
    "\n",
    "for epoch in range(training_args.num_train_epochs):\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        supplementary_data = batch[\"supplementary_data\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = peft_model(\n",
    "            input_ids=input_ids, supplementary_data=supplementary_data, labels=labels\n",
    "        )\n",
    "        loss = outputs[1]\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "01jClKYXdZ-Z"
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0FDkmqV7dXNf"
   },
   "outputs": [],
   "source": [
    "def prepare_inference_input(smiles, precursor_mz):\n",
    "    inputs = tokenizer(\n",
    "        smiles,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    inputs[\"supplementary_data\"] = torch.tensor(\n",
    "        [supplementary_data], dtype=torch.float\n",
    "    ).to(device)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "peft_model.eval()\n",
    "\n",
    "# Example data\n",
    "smiles_example = \"CCO\"\n",
    "supplementary_data_example = 0  # TODO\n",
    "\n",
    "# Prepare input\n",
    "inputs = prepare_inference_input(smiles_example, supplementary_data_example)\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    outputs = peft_model(**inputs)\n",
    "    logits = outputs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2D8bfVV9keeM"
   },
   "source": [
    "# Choices that affect the whole architecture\n",
    "\n",
    "*   Format for the supplementary data\n",
    "*   Format for the label data\n",
    "*   The format of the output of the new model\n",
    "\n",
    "\n",
    "\n",
    "### More modular choices (that are important)\n",
    "\n",
    "\n",
    "*   Whether we have to predict compound_class at inference\n",
    "*   Include in_silico data?\n",
    "*   Architeture of the modified ChemBERTa model\n",
    "*   LoRA parameters\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
