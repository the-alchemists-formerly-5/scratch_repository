{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"../data/raw/enveda_library_subset_10percent.parquet\"\n",
    "BASE_MODEL = \"seyonec/ChemBERTa-zinc-base-v1\"\n",
    "MAX_FRAGMENTS = 512 # from anton, max number of mzs/intensities\n",
    "MAX_SEQ_LENGTH = 512 # base model max seq length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tYhe3fk-sblS"
   },
   "source": [
    "# Messing around with ChemBERTa for fun and for education\n",
    "\n",
    "The first half of this colab is just fun experiments trying to understand ChemBERTa and it's tokenizer better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 420,
     "referenced_widgets": [
      "09447f7d805a456a8d9df123079fef9b",
      "491c2c5348da4cebb59112f76ff21867",
      "fe9ba8c3b518443ba11fd1e700dd2c24",
      "017803aafcb84942bd04ae4b6014d713",
      "7bb64ec2bc31458fac09dafbafc66030",
      "48ba3adb18ee45b085dcfcc74b8e4d19",
      "90c266cbe71d46aaade3ab541889fe07",
      "ed10bed1b375495cb383786278b88953",
      "0186504a6df04889965a532aeaa3058c",
      "9365963655bf4a6f9f65418af1213afb",
      "e8e0cfd2025e41cfb5ea102586988b28",
      "6b54217022de4316a071ee63832b77fe",
      "3cf79467e3384aeba7e957596b912e29",
      "d23dd09659b9428f8cc8083f44e04709",
      "a75c8346a8f74f43941aa43700e6b2d8",
      "e5968c80140b4a71978dbb453ee75af2",
      "285f3c7db7d448a7a9ed62b2f9324e53",
      "e02702dc96744f0bb3304606cfa29e24",
      "5810f7fa342f4543834d8274abe552f5",
      "433bfc6fb35a4199b1cedfbaf5f5a15e",
      "8d37ac015a0f4caebf5455c7c16284dc",
      "f3084e63613741098216632a5f92ef98",
      "1b99d732290a4190982895ff640ea719",
      "b284f3ac0854497fbab925eeb1ded0df",
      "01638020753146678ee0a7692603b6c2",
      "6cd3e39a24fa4ed490ea0567c51c6eee",
      "e0c994de1d654e92ac91d661932ebdfa",
      "2fa221bf24cb4bdf9695b5f593b0d818",
      "502de19d537d41a7b151ca5798f46a72",
      "b689e92305ae4b6693991b37e894f4a0",
      "50e84fe3afd94297a3f198d0dfc4654b",
      "dcc379d6713f47529ea18e30482cfa29",
      "6d6719e9bdcc411c95f60201ee37119a",
      "ec2a4d26292d4606bdcee5c2a0ea5f70",
      "cf7c0c5e780145cf90de13cd375c1cce",
      "6cc566ec625c4251908f083a4ac5e972",
      "ea2e9aa05204487fa884178986a59380",
      "6ff00156a8d1407e8a03899aede36f60",
      "57196d402589425baba759c1f92b3fee",
      "931340e0216b4cdd940a7041f14b62f1",
      "ccba3a5e8f3f4f08b79e5cad740b78b8",
      "f762dd67830c45a1b8725313b92346e4",
      "bc455b59103e4404b9f9b124d6999b3c",
      "d178a1d70c32435482c10d6824bf4131",
      "7de2b548546d4c58b4e1af83cdf618e7",
      "b5c94d24b105493796b8f70d4adb55a7",
      "be8b9e625b624ad89abb785ffdb352b9",
      "7f60ccd803d8490f990a58518b1aa4dd",
      "038ff99e2ffc4f7db7ecd54333a3d71e",
      "177f0b132f2f40b9b63a048635f0dc70",
      "47da5e41d88d4bc6bfb496adacddba30",
      "604c8bed2d6f4c04bb9ae2b78c6f4aea",
      "c83d02b54fe94d6889aae124641f43eb",
      "4df1d8407ea74a3193426b27a15eeaa9",
      "67e3bc2be1034a73be1fc0b3b97da027",
      "f53b7a64750849d18bb4df54237a1110",
      "f7b208a676664b59a90d7d38293df4dd",
      "504f51d204af44b7915060072ea359b2",
      "1f2793e435b04dc7b9502f643a98d533",
      "c9e22b771e9e49ad8354902723d9b951",
      "201c3ab1ff3a46128961e12e4edb2a01",
      "bd46618a5bce46ec840bae0adb001564",
      "ac00b65c79fc4b73a3e2d9d437001e3f",
      "d9c52550dcb8430a891ada649a35d5de",
      "4a99e037f63b4dbfa9d2f58912c46167",
      "f89668c6b88c4bad853403c95eed4fff"
     ]
    },
    "id": "KPjZH305wYrn",
    "outputId": "a36beb4d-b472-44c7-aed8-c200e1cc515c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at seyonec/ChemBERTa-zinc-base-v1 were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import (AutoModelForMaskedLM, AutoTokenizer)\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(BASE_MODEL)\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_cMXHTFeMOy"
   },
   "source": [
    "# Data prep\n",
    "\n",
    "\n",
    "These are the columns in the data set:\n",
    "\n",
    "precursor_mz - f64\n",
    "precursor_charge - f64\n",
    "mzs - list[f64]\n",
    "intensities - list[f64]\n",
    "in_silico - bool\n",
    "smiles - str\n",
    "adduct - str\n",
    "collision_energy - str\n",
    "instrument_type - str\n",
    "compound_class - str\n",
    "entropy - f64\n",
    "scaffold_smiles - str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "nyTWxIYveLsl",
    "outputId": "b83c7ecc-b236-47ff-9062-4dcb1e3db1f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['precursor_mz', 'precursor_charge', 'mzs', 'intensities', 'in_silico', 'smiles', 'adduct', 'collision_energy', 'instrument_type', 'compound_class', 'entropy', 'scaffold_smiles']\n",
      "['precursor_mz', 'precursor_charge', 'mzs', 'intensities', 'in_silico', 'smiles', 'adduct', 'collision_energy', 'instrument_type', 'compound_class', 'entropy', 'scaffold_smiles']\n",
      "shape: (5, 12)\n",
      "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬──────────┬───────────┐\n",
      "│ precursor ┆ precursor ┆ mzs       ┆ intensiti ┆ … ┆ instrumen ┆ compound_ ┆ entropy  ┆ scaffold_ │\n",
      "│ _mz       ┆ _charge   ┆ ---       ┆ es        ┆   ┆ t_type    ┆ class     ┆ ---      ┆ smiles    │\n",
      "│ ---       ┆ ---       ┆ list[f64] ┆ ---       ┆   ┆ ---       ┆ ---       ┆ f64      ┆ ---       │\n",
      "│ f64       ┆ f64       ┆           ┆ list[f64] ┆   ┆ str       ┆ str       ┆          ┆ str       │\n",
      "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪══════════╪═══════════╡\n",
      "│ 657.54634 ┆ -1.0      ┆ [41.00329 ┆ [0.002206 ┆ … ┆ cfm-predi ┆ Triacylgl ┆ 4.160459 ┆           │\n",
      "│ 9         ┆           ┆ ,         ┆ ,         ┆   ┆ ct 4      ┆ ycerols   ┆          ┆           │\n",
      "│           ┆           ┆ 43.01894, ┆ 0.14623,  ┆   ┆           ┆           ┆          ┆           │\n",
      "│           ┆           ┆ …         ┆ …         ┆   ┆           ┆           ┆          ┆           │\n",
      "│           ┆           ┆ 657.546…  ┆ 0.588434… ┆   ┆           ┆           ┆          ┆           │\n",
      "│ 827.71231 ┆ 1.0       ┆ [43.05423 ┆ [0.122531 ┆ … ┆ cfm-predi ┆ Triacylgl ┆ 4.856278 ┆           │\n",
      "│ 7         ┆           ┆ ,         ┆ ,         ┆   ┆ ct 4      ┆ ycerols   ┆          ┆           │\n",
      "│           ┆           ┆ 45.06988, ┆ 0.013169, ┆   ┆           ┆           ┆          ┆           │\n",
      "│           ┆           ┆ …         ┆ …         ┆   ┆           ┆           ┆          ┆           │\n",
      "│           ┆           ┆ 827.712…  ┆ 0.89753…  ┆   ┆           ┆           ┆          ┆           │\n",
      "│ 949.72796 ┆ 1.0       ┆ [39.02293 ┆ [0.012077 ┆ … ┆ cfm-predi ┆ Triacylgl ┆ 5.097924 ┆           │\n",
      "│ 7         ┆           ┆ ,         ┆ ,         ┆   ┆ ct 4      ┆ ycerols   ┆          ┆           │\n",
      "│           ┆           ┆ 41.03858, ┆ 0.064455, ┆   ┆           ┆           ┆          ┆           │\n",
      "│           ┆           ┆ …         ┆ …         ┆   ┆           ┆           ┆          ┆           │\n",
      "│           ┆           ┆ 949.727…  ┆ 0.86479…  ┆   ┆           ┆           ┆          ┆           │\n",
      "│ 925.85935 ┆ -1.0      ┆ [44.9982, ┆ [0.027488 ┆ … ┆ cfm-predi ┆ Triacylgl ┆ 4.282513 ┆           │\n",
      "│ 1         ┆           ┆ 47.01385, ┆ ,         ┆   ┆ ct 4      ┆ ycerols   ┆          ┆           │\n",
      "│           ┆           ┆ …         ┆ 0.006652, ┆   ┆           ┆           ┆          ┆           │\n",
      "│           ┆           ┆ 925.8593… ┆ …         ┆   ┆           ┆           ┆          ┆           │\n",
      "│           ┆           ┆           ┆ 0.60526…  ┆   ┆           ┆           ┆          ┆           │\n",
      "│ 357.30103 ┆ -1.0      ┆ [41.00329 ┆ [0.072312 ┆ … ┆ cfm-predi ┆ Monoacylg ┆ 3.550469 ┆           │\n",
      "│ 4         ┆           ┆ ,         ┆ ,         ┆   ┆ ct 4      ┆ lycerols  ┆          ┆           │\n",
      "│           ┆           ┆ 43.01894, ┆ 0.146965, ┆   ┆           ┆           ┆          ┆           │\n",
      "│           ┆           ┆ …         ┆ …         ┆   ┆           ┆           ┆          ┆           │\n",
      "│           ┆           ┆ 357.301…  ┆ 0.25859…  ┆   ┆           ┆           ┆          ┆           │\n",
      "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴──────────┴───────────┘\n",
      "shape: (5, 12)\n",
      "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬──────────┬───────────┐\n",
      "│ precursor ┆ precursor ┆ mzs       ┆ intensiti ┆ … ┆ instrumen ┆ compound_ ┆ entropy  ┆ scaffold_ │\n",
      "│ _mz       ┆ _charge   ┆ ---       ┆ es        ┆   ┆ t_type    ┆ class     ┆ ---      ┆ smiles    │\n",
      "│ ---       ┆ ---       ┆ list[f64] ┆ ---       ┆   ┆ ---       ┆ ---       ┆ f64      ┆ ---       │\n",
      "│ f64       ┆ f64       ┆           ┆ list[f64] ┆   ┆ str       ┆ str       ┆          ┆ str       │\n",
      "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪══════════╪═══════════╡\n",
      "│ 527.13365 ┆ 1.0       ┆ [41.00219 ┆ [0.010587 ┆ … ┆ cfm-predi ┆ Chalcones ┆ 3.58425  ┆ O=C(CC(c1 │\n",
      "│ 8         ┆           ┆ ,         ┆ ,         ┆   ┆ ct 4      ┆           ┆          ┆ ccccc1)c1 │\n",
      "│           ┆           ┆ 43.01784, ┆ 0.023416, ┆   ┆           ┆           ┆          ┆ c(-c2cccc │\n",
      "│           ┆           ┆ …         ┆ …         ┆   ┆           ┆           ┆          ┆ c2)…      │\n",
      "│           ┆           ┆ 527.133…  ┆ 0.87760…  ┆   ┆           ┆           ┆          ┆           │\n",
      "│ 866.46180 ┆ 1.0       ┆ [41.03858 ┆ [0.15955, ┆ … ┆ cfm-predi ┆ Cyclic    ┆ 4.035891 ┆ O=C(CC12C │\n",
      "│ 9         ┆           ┆ ,         ┆ 0.176604, ┆   ┆ ct 4      ┆ peptides  ┆          ┆ CC(CO1)CO │\n",
      "│           ┆           ┆ 42.03383, ┆ … 1.0]    ┆   ┆           ┆           ┆          ┆ 2)NC1COC( │\n",
      "│           ┆           ┆ …         ┆           ┆   ┆           ┆           ┆          ┆ =O)…      │\n",
      "│           ┆           ┆ 866.461…  ┆           ┆   ┆           ┆           ┆          ┆           │\n",
      "│ 325.04844 ┆ -1.0      ┆ [34.9694, ┆ [0.335189 ┆ … ┆ cfm-predi ┆ Meroterpe ┆ 4.01902  ┆ O=C(CC12C │\n",
      "│           ┆           ┆ 41.00329, ┆ ,         ┆   ┆ ct 4      ┆ noids     ┆          ┆ CCC1COC2= │\n",
      "│           ┆           ┆ …         ┆ 0.514464, ┆   ┆           ┆ with      ┆          ┆ O)c1ccccc │\n",
      "│           ┆           ┆ 325.0484… ┆ … 1.0]    ┆   ┆           ┆ bridged   ┆          ┆ 1         │\n",
      "│           ┆           ┆           ┆           ┆   ┆           ┆ ri…       ┆          ┆           │\n",
      "│ 583.33777 ┆ 1.0       ┆ [30.03383 ┆ [0.009245 ┆ … ┆ cfm-predi ┆ Terpenoid ┆ 4.019998 ┆ O=C(CC12C │\n",
      "│ 8         ┆           ┆ ,         ┆ ,         ┆   ┆ ct 4      ┆ alkaloids ┆          ┆ CCC34C5CC │\n",
      "│           ┆           ┆ 32.04948, ┆ 0.007699, ┆   ┆           ┆           ┆          ┆ 6CCC(C5C6 │\n",
      "│           ┆           ┆ …         ┆ … 1.0]    ┆   ┆           ┆           ┆          ┆ )C(…      │\n",
      "│           ┆           ┆ 583.337…  ┆           ┆   ┆           ┆           ┆          ┆           │\n",
      "│ 679.40628 ┆ -1.0      ┆ [41.00329 ┆ [0.074812 ┆ … ┆ cfm-predi ┆ Ursane    ┆ 3.841568 ┆ O=C(CC12C │\n",
      "│ 7         ┆           ┆ ,         ┆ ,         ┆   ┆ ct 4      ┆ and Tarax ┆          ┆ CCCC1C1=C │\n",
      "│           ┆           ┆ 43.01894, ┆ 0.090091, ┆   ┆           ┆ astane    ┆          ┆ CC3C4CCCC │\n",
      "│           ┆           ┆ …         ┆ … 1.0]    ┆   ┆           ┆ triterp…  ┆          ┆ C4C…      │\n",
      "│           ┆           ┆ 679.406…  ┆           ┆   ┆           ┆           ┆          ┆           │\n",
      "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴──────────┴───────────┘\n",
      "Loading tokenizer\n",
      "Transforming raw data and writing prepared data to disk\n",
      "Reading prepared data from disk\n",
      "Loading tokenizer\n",
      "Transforming raw data and writing prepared data to disk\n",
      "Reading prepared data from disk\n"
     ]
    }
   ],
   "source": [
    "# import the data (with pandas?)\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "## Load the dataset (for some reason this didn't work for me)\n",
    "# df = pd.read_parquet('enveda_library_subset 2.parquet')\n",
    "\n",
    "# print(df.head())\n",
    "\n",
    "\n",
    "# custom Dataset class for all the types of data.\n",
    "# I think we might want to make a new 'column' of data that combines mzs and intensities into \"label\"\n",
    "\n",
    "from src.team5.data.data_loader import SMILESDataset\n",
    "from src.team5.data.data_split import sort_dataframe_by_scaffold, split_dataframe\n",
    "from src.team5.data.prepare import tensorize\n",
    "\n",
    "df = pl.read_parquet(DATASET)\n",
    "\n",
    "df_sorted = sort_dataframe_by_scaffold(df)\n",
    "\n",
    "df_train, df_test = split_dataframe(df_sorted, split_ratio=0.9)\n",
    "\n",
    "# Print column names\n",
    "print(df_train.columns)\n",
    "print(df_test.columns)\n",
    "print(df_train.head())\n",
    "print(df_test.head())\n",
    "\n",
    "(train_tokenized_smiles, train_attention_mask, train_labels, train_supplementary_data) = tensorize(df_train)\n",
    "(test_tokenized_smiles, test_attention_mask, test_labels, test_supplementary_data) = tensorize(df_test)\n",
    "\n",
    "train_dataset = SMILESDataset(train_tokenized_smiles, train_attention_mask, train_labels, train_supplementary_data)\n",
    "test_dataset = SMILESDataset(test_tokenized_smiles, test_attention_mask, test_labels, test_supplementary_data)\n",
    "\n",
    "## batch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': torch.int64, 'attention_mask': torch.int64, 'labels': torch.float32, 'supplementary_data': torch.float32}\n"
     ]
    }
   ],
   "source": [
    "print({k:v.dtype for k,v in train_dataset[0].items()})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fENovYIHeaOO"
   },
   "source": [
    "# Custom model for our problem\n",
    "This is probably the most important part in terms of design choices. We are changing the ChemBERTa model by adding on something at the end. This new module will take the hidden SMILES embedding from the last hidden layer as input. It will also take in all the other data about the precusor molecule and experimental conditions (eg, precusor mz, collison energy etc). For now, let's call that supplementary data.\n",
    "\n",
    "I've written the simplest possible thing here: a single linear layer that takes the embedding of the entire seq, concatinated with all the supplementary data for the example. It outputs \"labels\", which is mzs and intensities zipped together.\n",
    "\n",
    "The reason for making a single module output both mzs and intensities is because there needs to be the same number of fragments per example, and the two numbers are very related.\n",
    "\n",
    "A single linear layer is probably a terrible choice though, since this is the only layer that sees all the supplementary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "rMogXqZteZ1Z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomChemBERTaModel(\n",
      "  (model): RobertaForMaskedLM(\n",
      "    (roberta): RobertaModel(\n",
      "      (embeddings): RobertaEmbeddings(\n",
      "        (word_embeddings): Embedding(767, 768, padding_idx=1)\n",
      "        (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "        (token_type_embeddings): Embedding(1, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): RobertaEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0-5): 6 x RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSdpaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (lm_head): RobertaLMHead(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (decoder): Linear(in_features=768, out_features=767, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (final_layers): FinalLayers(\n",
      "    (layer1): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (activation1): ReLU()\n",
      "    (layer2): Linear(in_features=843, out_features=16, bias=True)\n",
      "    (layer3): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "  )\n",
      ")\n",
      "model.roberta.embeddings.word_embeddings.weight has shape torch.Size([767, 768])\n",
      "model.roberta.embeddings.position_embeddings.weight has shape torch.Size([514, 768])\n",
      "model.roberta.embeddings.token_type_embeddings.weight has shape torch.Size([1, 768])\n",
      "model.roberta.embeddings.LayerNorm.weight has shape torch.Size([768])\n",
      "model.roberta.embeddings.LayerNorm.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.0.attention.self.query.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.0.attention.self.query.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.0.attention.self.key.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.0.attention.self.key.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.0.attention.self.value.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.0.attention.self.value.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.0.attention.output.dense.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.0.attention.output.dense.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.0.attention.output.LayerNorm.weight has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.0.attention.output.LayerNorm.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.0.intermediate.dense.weight has shape torch.Size([3072, 768])\n",
      "model.roberta.encoder.layer.0.intermediate.dense.bias has shape torch.Size([3072])\n",
      "model.roberta.encoder.layer.0.output.dense.weight has shape torch.Size([768, 3072])\n",
      "model.roberta.encoder.layer.0.output.dense.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.0.output.LayerNorm.weight has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.0.output.LayerNorm.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.1.attention.self.query.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.1.attention.self.query.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.1.attention.self.key.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.1.attention.self.key.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.1.attention.self.value.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.1.attention.self.value.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.1.attention.output.dense.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.1.attention.output.dense.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.1.attention.output.LayerNorm.weight has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.1.attention.output.LayerNorm.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.1.intermediate.dense.weight has shape torch.Size([3072, 768])\n",
      "model.roberta.encoder.layer.1.intermediate.dense.bias has shape torch.Size([3072])\n",
      "model.roberta.encoder.layer.1.output.dense.weight has shape torch.Size([768, 3072])\n",
      "model.roberta.encoder.layer.1.output.dense.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.1.output.LayerNorm.weight has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.1.output.LayerNorm.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.2.attention.self.query.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.2.attention.self.query.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.2.attention.self.key.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.2.attention.self.key.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.2.attention.self.value.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.2.attention.self.value.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.2.attention.output.dense.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.2.attention.output.dense.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.2.attention.output.LayerNorm.weight has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.2.attention.output.LayerNorm.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.2.intermediate.dense.weight has shape torch.Size([3072, 768])\n",
      "model.roberta.encoder.layer.2.intermediate.dense.bias has shape torch.Size([3072])\n",
      "model.roberta.encoder.layer.2.output.dense.weight has shape torch.Size([768, 3072])\n",
      "model.roberta.encoder.layer.2.output.dense.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.2.output.LayerNorm.weight has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.2.output.LayerNorm.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.3.attention.self.query.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.3.attention.self.query.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.3.attention.self.key.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.3.attention.self.key.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.3.attention.self.value.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.3.attention.self.value.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.3.attention.output.dense.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.3.attention.output.dense.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.3.attention.output.LayerNorm.weight has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.3.attention.output.LayerNorm.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.3.intermediate.dense.weight has shape torch.Size([3072, 768])\n",
      "model.roberta.encoder.layer.3.intermediate.dense.bias has shape torch.Size([3072])\n",
      "model.roberta.encoder.layer.3.output.dense.weight has shape torch.Size([768, 3072])\n",
      "model.roberta.encoder.layer.3.output.dense.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.3.output.LayerNorm.weight has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.3.output.LayerNorm.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.4.attention.self.query.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.4.attention.self.query.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.4.attention.self.key.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.4.attention.self.key.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.4.attention.self.value.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.4.attention.self.value.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.4.attention.output.dense.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.4.attention.output.dense.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.4.attention.output.LayerNorm.weight has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.4.attention.output.LayerNorm.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.4.intermediate.dense.weight has shape torch.Size([3072, 768])\n",
      "model.roberta.encoder.layer.4.intermediate.dense.bias has shape torch.Size([3072])\n",
      "model.roberta.encoder.layer.4.output.dense.weight has shape torch.Size([768, 3072])\n",
      "model.roberta.encoder.layer.4.output.dense.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.4.output.LayerNorm.weight has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.4.output.LayerNorm.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.5.attention.self.query.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.5.attention.self.query.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.5.attention.self.key.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.5.attention.self.key.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.5.attention.self.value.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.5.attention.self.value.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.5.attention.output.dense.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.5.attention.output.dense.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.5.attention.output.LayerNorm.weight has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.5.attention.output.LayerNorm.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.5.intermediate.dense.weight has shape torch.Size([3072, 768])\n",
      "model.roberta.encoder.layer.5.intermediate.dense.bias has shape torch.Size([3072])\n",
      "model.roberta.encoder.layer.5.output.dense.weight has shape torch.Size([768, 3072])\n",
      "model.roberta.encoder.layer.5.output.dense.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.5.output.LayerNorm.weight has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.5.output.LayerNorm.bias has shape torch.Size([768])\n",
      "model.lm_head.bias has shape torch.Size([767])\n",
      "model.lm_head.dense.weight has shape torch.Size([768, 768])\n",
      "model.lm_head.dense.bias has shape torch.Size([768])\n",
      "model.lm_head.layer_norm.weight has shape torch.Size([768])\n",
      "model.lm_head.layer_norm.bias has shape torch.Size([768])\n",
      "final_layers.layer1.weight has shape torch.Size([128, 512])\n",
      "final_layers.layer1.bias has shape torch.Size([128])\n",
      "final_layers.layer2.weight has shape torch.Size([16, 843])\n",
      "final_layers.layer2.bias has shape torch.Size([16])\n",
      "final_layers.layer3.weight has shape torch.Size([1024, 2048])\n",
      "final_layers.layer3.bias has shape torch.Size([1024])\n"
     ]
    }
   ],
   "source": [
    "from src.team5.models.custom_model import CustomChemBERTaModel\n",
    "\n",
    "MS_model = CustomChemBERTaModel(model, MAX_FRAGMENTS, MAX_SEQ_LENGTH)\n",
    "\n",
    "print(MS_model)\n",
    "\n",
    "for name, param in MS_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name} has shape {param.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IDwMMYCaYAZK"
   },
   "source": [
    "# LoRA config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "y0PF6-qGZ683"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,324,800 || all params: 48,608,383 || trainable%: 4.7827\n",
      "base_model.model.model.roberta.encoder.layer.0.attention.self.query.lora_A.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.0.attention.self.query.lora_B.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.0.attention.self.value.lora_A.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.0.attention.self.value.lora_B.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.1.attention.self.query.lora_A.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.1.attention.self.query.lora_B.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.1.attention.self.value.lora_A.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.1.attention.self.value.lora_B.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.2.attention.self.query.lora_A.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.2.attention.self.query.lora_B.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.2.attention.self.value.lora_A.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.2.attention.self.value.lora_B.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.3.attention.self.query.lora_A.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.3.attention.self.query.lora_B.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.3.attention.self.value.lora_A.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.3.attention.self.value.lora_B.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.4.attention.self.query.lora_A.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.4.attention.self.query.lora_B.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.4.attention.self.value.lora_A.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.4.attention.self.value.lora_B.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.5.attention.self.query.lora_A.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.5.attention.self.query.lora_B.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.5.attention.self.value.lora_A.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.5.attention.self.value.lora_B.default.weight is trainable\n",
      "base_model.model.final_layers.modules_to_save.default.layer1.weight is trainable\n",
      "base_model.model.final_layers.modules_to_save.default.layer1.bias is trainable\n",
      "base_model.model.final_layers.modules_to_save.default.layer2.weight is trainable\n",
      "base_model.model.final_layers.modules_to_save.default.layer2.bias is trainable\n",
      "base_model.model.final_layers.modules_to_save.default.layer3.weight is trainable\n",
      "base_model.model.final_layers.modules_to_save.default.layer3.bias is trainable\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"query\", \"value\"],\n",
    "    modules_to_save=[\n",
    "        \"final_layers\"\n",
    "    ],  # change this to the name of the new modules at the end.\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(MS_model, peft_config)\n",
    "\n",
    "peft_model.print_trainable_parameters()  # check that it's training the right things\n",
    "for name, param in peft_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name} is trainable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-q-zpo0kaF0p"
   },
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lars/workspace/scratch_repository/.venv/lib/python3.12/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2176bae70f854bf3a4f6deff6af170dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "a Tensor with 4096 elements cannot be converted to Scalar",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 24\u001b[0m\n\u001b[1;32m      8\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m      9\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../logs/test_trainer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     report_to\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     17\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     18\u001b[0m     model\u001b[38;5;241m=\u001b[39mpeft_model,\n\u001b[1;32m     19\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m     20\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,\n\u001b[1;32m     21\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mtest_dataset,\n\u001b[1;32m     22\u001b[0m )\n\u001b[0;32m---> 24\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/scratch_repository/.venv/lib/python3.12/site-packages/transformers/trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/scratch_repository/.venv/lib/python3.12/site-packages/transformers/trainer.py:2388\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2387\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2388\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2391\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2392\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2393\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2394\u001b[0m ):\n\u001b[1;32m   2395\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2396\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/workspace/scratch_repository/.venv/lib/python3.12/site-packages/transformers/trainer.py:3485\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3484\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3485\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3487\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3489\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3490\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3491\u001b[0m ):\n",
      "File \u001b[0;32m~/workspace/scratch_repository/.venv/lib/python3.12/site-packages/transformers/trainer.py:3532\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3530\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3531\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3532\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3533\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3534\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/workspace/scratch_repository/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/scratch_repository/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/scratch_repository/.venv/lib/python3.12/site-packages/peft/peft_model.py:812\u001b[0m, in \u001b[0;36mPeftModel.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    811\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m--> 812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_base_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/scratch_repository/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/scratch_repository/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/scratch_repository/src/team5/models/custom_model.py:160\u001b[0m, in \u001b[0;36mCustomChemBERTaModel.forward\u001b[0;34m(self, input_ids, attention_mask, supplementary_data, labels)\u001b[0m\n\u001b[1;32m    157\u001b[0m predicted_output \u001b[38;5;241m=\u001b[39m predicted_output\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# Calculate the loss by comparing to labels\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredicted_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss, predicted_output\n",
      "File \u001b[0;32m~/workspace/scratch_repository/src/team5/models/custom_model.py:45\u001b[0m, in \u001b[0;36mcalculate_loss\u001b[0;34m(predicted_output, labels)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_loss\u001b[39m(predicted_output, labels):\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgreedy_cosine_similarity_for_interleaved\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredicted_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/scratch_repository/src/team5/models/custom_model.py:104\u001b[0m, in \u001b[0;36mgreedy_cosine_similarity_for_interleaved\u001b[0;34m(pred_vec, actual_vec)\u001b[0m\n\u001b[1;32m    101\u001b[0m sorted_actual_mzs, sorted_actual_intensities \u001b[38;5;241m=\u001b[39m sort_two_by_first(actual_mzs, actual_intensities)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# Compute cosine similarity for m/z and intensities separately after sorting by m/z\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m similarity_mzs_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mabs\u001b[39m(\u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43msorted_pred_mzs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msorted_actual_mzs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    105\u001b[0m similarity_intensities_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mabs\u001b[39m(cosine_similarity(sorted_pred_intensities\u001b[38;5;241m.\u001b[39mfloat(), sorted_actual_intensities\u001b[38;5;241m.\u001b[39mfloat(), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m    106\u001b[0m similarity_1 \u001b[38;5;241m=\u001b[39m (similarity_mzs_1 \u001b[38;5;241m+\u001b[39m similarity_intensities_1) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: a Tensor with 4096 elements cannot be converted to Scalar"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "import torch\n",
    "from transformers import Trainer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "peft_model.to(device)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../logs/test_trainer\",\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=0.1,\n",
    "    label_names=[\"labels\"],\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "01jClKYXdZ-Z"
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0FDkmqV7dXNf"
   },
   "outputs": [],
   "source": [
    "def prepare_inference_input(smiles, precursor_mz):\n",
    "    inputs = tokenizer(\n",
    "        smiles,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    inputs[\"supplementary_data\"] = torch.tensor(\n",
    "        [supplementary_data], dtype=torch.float\n",
    "    ).to(device)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "peft_model.eval()\n",
    "\n",
    "# Example data\n",
    "smiles_example = \"CCO\"\n",
    "supplementary_data_example = 0  # TODO\n",
    "\n",
    "# Prepare input\n",
    "inputs = prepare_inference_input(smiles_example, supplementary_data_example)\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    outputs = peft_model(**inputs)\n",
    "    logits = outputs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2D8bfVV9keeM"
   },
   "source": [
    "# Choices that affect the whole architecture\n",
    "\n",
    "*   Format for the supplementary data\n",
    "*   Format for the label data\n",
    "*   The format of the output of the new model\n",
    "\n",
    "\n",
    "\n",
    "### More modular choices (that are important)\n",
    "\n",
    "\n",
    "*   Whether we have to predict compound_class at inference\n",
    "*   Include in_silico data?\n",
    "*   Architeture of the modified ChemBERTa model\n",
    "*   LoRA parameters\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
