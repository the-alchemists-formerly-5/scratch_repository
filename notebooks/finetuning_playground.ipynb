{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"../data/raw/enveda_library_subset_10percent.parquet\"\n",
    "MAX_FRAGMENTS = 512 # from anton, max number of mzs/intensities\n",
    "MAX_SEQ_LENGTH = 1024 # from anton, max length of SMILES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tYhe3fk-sblS"
   },
   "source": [
    "# Messing around with ChemBERTa for fun and for education\n",
    "\n",
    "The first half of this colab is just fun experiments trying to understand ChemBERTa and it's tokenizer better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 420,
     "referenced_widgets": [
      "09447f7d805a456a8d9df123079fef9b",
      "491c2c5348da4cebb59112f76ff21867",
      "fe9ba8c3b518443ba11fd1e700dd2c24",
      "017803aafcb84942bd04ae4b6014d713",
      "7bb64ec2bc31458fac09dafbafc66030",
      "48ba3adb18ee45b085dcfcc74b8e4d19",
      "90c266cbe71d46aaade3ab541889fe07",
      "ed10bed1b375495cb383786278b88953",
      "0186504a6df04889965a532aeaa3058c",
      "9365963655bf4a6f9f65418af1213afb",
      "e8e0cfd2025e41cfb5ea102586988b28",
      "6b54217022de4316a071ee63832b77fe",
      "3cf79467e3384aeba7e957596b912e29",
      "d23dd09659b9428f8cc8083f44e04709",
      "a75c8346a8f74f43941aa43700e6b2d8",
      "e5968c80140b4a71978dbb453ee75af2",
      "285f3c7db7d448a7a9ed62b2f9324e53",
      "e02702dc96744f0bb3304606cfa29e24",
      "5810f7fa342f4543834d8274abe552f5",
      "433bfc6fb35a4199b1cedfbaf5f5a15e",
      "8d37ac015a0f4caebf5455c7c16284dc",
      "f3084e63613741098216632a5f92ef98",
      "1b99d732290a4190982895ff640ea719",
      "b284f3ac0854497fbab925eeb1ded0df",
      "01638020753146678ee0a7692603b6c2",
      "6cd3e39a24fa4ed490ea0567c51c6eee",
      "e0c994de1d654e92ac91d661932ebdfa",
      "2fa221bf24cb4bdf9695b5f593b0d818",
      "502de19d537d41a7b151ca5798f46a72",
      "b689e92305ae4b6693991b37e894f4a0",
      "50e84fe3afd94297a3f198d0dfc4654b",
      "dcc379d6713f47529ea18e30482cfa29",
      "6d6719e9bdcc411c95f60201ee37119a",
      "ec2a4d26292d4606bdcee5c2a0ea5f70",
      "cf7c0c5e780145cf90de13cd375c1cce",
      "6cc566ec625c4251908f083a4ac5e972",
      "ea2e9aa05204487fa884178986a59380",
      "6ff00156a8d1407e8a03899aede36f60",
      "57196d402589425baba759c1f92b3fee",
      "931340e0216b4cdd940a7041f14b62f1",
      "ccba3a5e8f3f4f08b79e5cad740b78b8",
      "f762dd67830c45a1b8725313b92346e4",
      "bc455b59103e4404b9f9b124d6999b3c",
      "d178a1d70c32435482c10d6824bf4131",
      "7de2b548546d4c58b4e1af83cdf618e7",
      "b5c94d24b105493796b8f70d4adb55a7",
      "be8b9e625b624ad89abb785ffdb352b9",
      "7f60ccd803d8490f990a58518b1aa4dd",
      "038ff99e2ffc4f7db7ecd54333a3d71e",
      "177f0b132f2f40b9b63a048635f0dc70",
      "47da5e41d88d4bc6bfb496adacddba30",
      "604c8bed2d6f4c04bb9ae2b78c6f4aea",
      "c83d02b54fe94d6889aae124641f43eb",
      "4df1d8407ea74a3193426b27a15eeaa9",
      "67e3bc2be1034a73be1fc0b3b97da027",
      "f53b7a64750849d18bb4df54237a1110",
      "f7b208a676664b59a90d7d38293df4dd",
      "504f51d204af44b7915060072ea359b2",
      "1f2793e435b04dc7b9502f643a98d533",
      "c9e22b771e9e49ad8354902723d9b951",
      "201c3ab1ff3a46128961e12e4edb2a01",
      "bd46618a5bce46ec840bae0adb001564",
      "ac00b65c79fc4b73a3e2d9d437001e3f",
      "d9c52550dcb8430a891ada649a35d5de",
      "4a99e037f63b4dbfa9d2f58912c46167",
      "f89668c6b88c4bad853403c95eed4fff"
     ]
    },
    "id": "KPjZH305wYrn",
    "outputId": "a36beb4d-b472-44c7-aed8-c200e1cc515c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at seyonec/PubChem10M_SMILES_BPE_450k were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import (AutoModelForMaskedLM, AutoTokenizer, RobertaModel,\n",
    "                          RobertaTokenizer, pipeline)\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"seyonec/PubChem10M_SMILES_BPE_450k\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"seyonec/PubChem10M_SMILES_BPE_450k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_cMXHTFeMOy"
   },
   "source": [
    "# Data prep\n",
    "\n",
    "\n",
    "These are the columns in the data set:\n",
    "\n",
    "precursor_mz - f64\n",
    "precursor_charge - f64\n",
    "mzs - list[f64]\n",
    "intensities - list[f64]\n",
    "in_silico - bool\n",
    "smiles - str\n",
    "adduct - str\n",
    "collision_energy - str\n",
    "instrument_type - str\n",
    "compound_class - str\n",
    "entropy - f64\n",
    "scaffold_smiles - str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "nyTWxIYveLsl",
    "outputId": "b83c7ecc-b236-47ff-9062-4dcb1e3db1f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['precursor_mz', 'precursor_charge', 'mzs', 'intensities', 'in_silico',\n",
      "       'smiles', 'adduct', 'collision_energy', 'instrument_type',\n",
      "       'compound_class', 'entropy', 'scaffold_smiles'],\n",
      "      dtype='object')\n",
      "Index(['precursor_mz', 'precursor_charge', 'mzs', 'intensities', 'in_silico',\n",
      "       'smiles', 'adduct', 'collision_energy', 'instrument_type',\n",
      "       'compound_class', 'entropy', 'scaffold_smiles'],\n",
      "      dtype='object')\n",
      "        precursor_mz  precursor_charge  \\\n",
      "39785     470.145655              -1.0   \n",
      "42499     439.088200              -1.0   \n",
      "100599    947.900117               1.0   \n",
      "17805     521.310894               1.0   \n",
      "6095      407.186398              -1.0   \n",
      "\n",
      "                                                      mzs  \\\n",
      "39785   [41.00329, 43.01894, 45.03459, 53.00329, 55.01...   \n",
      "42499   [41.00329, 43.01894, 44.9982, 47.01385, 49.008...   \n",
      "100599  [43.05423, 55.05423, 57.06988, 59.08553, 67.05...   \n",
      "17805   [41.03858, 43.01784, 43.05423, 45.03349, 47.04...   \n",
      "6095    [39.02402, 41.00329, 41.03967, 43.01894, 55.01...   \n",
      "\n",
      "                                              intensities  in_silico  \\\n",
      "39785   [0.018890435474249352, 0.07034201630542851, 0....       True   \n",
      "42499   [0.06974122721035092, 0.014359929425602823, 0....       True   \n",
      "100599  [0.16971062037898443, 0.5443862312099104, 0.89...       True   \n",
      "17805   [0.01274466986937706, 0.42688688609203157, 0.0...       True   \n",
      "6095    [0.028191631724507088, 0.02917298599339816, 0....       True   \n",
      "\n",
      "                                                   smiles  adduct  \\\n",
      "39785   COc1cc2ccnc(Cc3ccc4c(c3)OCO4)c2cc1OC1OC(CO)C(O...  [M-H]-   \n",
      "42499   O=C(O)CC(=O)OC1CC(O)(C(=O)O)CC(O)C1OC(=O)C=Cc1...  [M-H]-   \n",
      "100599  CCCCCCCCCCCCCCCCCC(=O)OCC(COC(=O)CCCCCCCCCCCCC...  [M+H]+   \n",
      "17805   C=C1C(O)CCC2(C)CC(OC(C)=O)C3=C(C)CC(OC(=O)C(C)...  [M+H]+   \n",
      "6095    C=C(C)C(O)Cc1cc(C(=O)C=Cc2ccc(O)c(CC=C(C)C)c2)...  [M-H]-   \n",
      "\n",
      "       collision_energy instrument_type                  compound_class  \\\n",
      "39785          10-20-40   cfm-predict 4                            None   \n",
      "42499          10-20-40   cfm-predict 4  Cinnamic acids and derivatives   \n",
      "100599         10-20-40   cfm-predict 4                Triacylglycerols   \n",
      "17805          10-20-40   cfm-predict 4             Taxane diterpenoids   \n",
      "6095           10-20-40   cfm-predict 4                       Chalcones   \n",
      "\n",
      "         entropy                              scaffold_smiles  \n",
      "39785   3.558116  c1cc2ccc(OC3CCCCO3)cc2c(Cc2ccc3c(c2)OCO3)n1  \n",
      "42499   3.477090                    O=C(C=Cc1ccccc1)OC1CCCCC1  \n",
      "100599  4.826215                                               \n",
      "17805   3.802494                   C=C1CCCC2CCC3=CCCC(C3)CC12  \n",
      "6095    4.469193                     O=C(C=Cc1ccccc1)c1ccccc1  \n",
      "       precursor_mz  precursor_charge  \\\n",
      "22853    482.295000              -1.0   \n",
      "95609    823.447447               1.0   \n",
      "86504    268.104000               1.0   \n",
      "27803    251.091400               1.0   \n",
      "45044    549.342194               1.0   \n",
      "\n",
      "                                                     mzs  \\\n",
      "22853  [79.956841, 80.96534, 106.980797, 124.00724, 4...   \n",
      "95609  [39.02293, 43.01784, 45.03349, 47.04914, 57.03...   \n",
      "86504   [85.0283, 119.0348, 133.0496, 136.0618, 268.104]   \n",
      "27803  [39.02293, 41.00219, 41.03858, 43.01784, 57.03...   \n",
      "45044  [39.02293, 41.00219, 41.03858, 43.01784, 43.05...   \n",
      "\n",
      "                                             intensities  in_silico  \\\n",
      "22853  [0.023, 0.011000000000000001, 0.01100000000000...      False   \n",
      "95609  [0.0072682926829268305, 0.014634146341463415, ...       True   \n",
      "86504  [0.001001001001001001, 0.001001001001001001, 0...      False   \n",
      "27803  [0.4510985116938342, 0.0874822820694543, 0.014...       True   \n",
      "45044  [0.015864774431752625, 0.006634360216914735, 0...       True   \n",
      "\n",
      "                                                  smiles  adduct  \\\n",
      "22853  CC(CCC(=O)NCCS(=O)(=O)O)C1CCC2C3CCC4CC(O)CCC4(...  [M-H]-   \n",
      "95609  CC(=O)OC1CC2(O)C3CCC4CC(OC5CC(O)C(OC6CC(O)C(OC...  [M+H]+   \n",
      "86504                      Nc1ncnc2c1ncn2C1OC(CO)C(O)C1O  [M+H]+   \n",
      "27803                 CC(=O)C1c2cc(O)cc(O)c2C(=O)CC1(C)O  [M+H]+   \n",
      "45044  COC(=O)C(C)CC(=O)CC(C)C1CC(O)C2(C)C3C(O)CC4C(C...  [M+H]+   \n",
      "\n",
      "      collision_energy instrument_type  \\\n",
      "22853             None            qTof   \n",
      "95609         10-20-40   cfm-predict 4   \n",
      "86504     15 (nominal)      LC-ESI-QFT   \n",
      "27803         10-20-40   cfm-predict 4   \n",
      "45044         10-20-40   cfm-predict 4   \n",
      "\n",
      "                                         compound_class   entropy  \\\n",
      "22853                                  Cholane steroids  0.311068   \n",
      "95609                                      Cardenolides  4.541760   \n",
      "86504                             Purine nucleos(t)ides  0.662872   \n",
      "27803                                    Naphthalenones  3.184302   \n",
      "45044  Lanostane, Tirucallane and Euphane triterpenoids  4.412061   \n",
      "\n",
      "                                         scaffold_smiles  \n",
      "22853                        C1CCC2C(C1)CCC1C3CCCC3CCC21  \n",
      "95609  O=C1C=C(C2CCC3C2CCC2C4CCC(OC5CCC(OC6CCC(OC7CCC...  \n",
      "86504                             c1ncc2ncn(C3CCCO3)c2n1  \n",
      "27803                                   O=C1CCCc2ccccc21  \n",
      "45044                       O=C1CC2CCCC2C2=C1C1CCCCC1CC2  \n",
      "93474\n",
      "10386\n"
     ]
    }
   ],
   "source": [
    "# import the data (with pandas?)\n",
    "import polars as pl\n",
    "\n",
    "## Load the dataset (for some reason this didn't work for me)\n",
    "# df = pd.read_parquet('enveda_library_subset 2.parquet')\n",
    "\n",
    "# print(df.head())\n",
    "\n",
    "\n",
    "# tokenize the SMILES. Do we need to pad? If so, what's the max length\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"smiles\"], truncation=True, padding=\"max_length\", max_length=128\n",
    "    )\n",
    "\n",
    "\n",
    "# custom Dataset class for all the types of data.\n",
    "# I think we might want to make a new 'column' of data that combines mzs and intensities into \"label\"\n",
    "\n",
    "from src.team5.data.data_loader import SMILESDataset\n",
    "from src.team5.data.data_split import split_data\n",
    "\n",
    "df = pl.read_parquet(DATASET).to_pandas()\n",
    "\n",
    "train_df, eval_df = split_data(df)\n",
    "\n",
    "# Print column names\n",
    "print(train_df.columns)\n",
    "print(eval_df.columns)\n",
    "print(train_df.head())\n",
    "print(eval_df.head())\n",
    "\n",
    "train_dataset = SMILESDataset(train_df, tokenizer, MAX_SEQ_LENGTH)\n",
    "eval_dataset = SMILESDataset(eval_df, tokenizer, MAX_SEQ_LENGTH)\n",
    "\n",
    "# Print length of datasets\n",
    "print(len(train_dataset))\n",
    "print(len(eval_dataset))\n",
    "\n",
    "## batch?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fENovYIHeaOO"
   },
   "source": [
    "# Custom model for our problem\n",
    "This is probably the most important part in terms of design choices. We are changing the ChemBERTa model by adding on something at the end. This new module will take the hidden SMILES embedding from the last hidden layer as input. It will also take in all the other data about the precusor molecule and experimental conditions (eg, precusor mz, collison energy etc). For now, let's call that supplementary data.\n",
    "\n",
    "I've written the simplest possible thing here: a single linear layer that takes the embedding of the entire seq, concatinated with all the supplementary data for the example. It outputs \"labels\", which is mzs and intensities zipped together.\n",
    "\n",
    "The reason for making a single module output both mzs and intensities is because there needs to be the same number of fragments per example, and the two numbers are very related.\n",
    "\n",
    "A single linear layer is probably a terrible choice though, since this is the only layer that sees all the supplementary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "rMogXqZteZ1Z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomChemBERTaModel(\n",
      "  (model): RobertaForMaskedLM(\n",
      "    (roberta): RobertaModel(\n",
      "      (embeddings): RobertaEmbeddings(\n",
      "        (word_embeddings): Embedding(52000, 768, padding_idx=1)\n",
      "        (position_embeddings): Embedding(512, 768, padding_idx=1)\n",
      "        (token_type_embeddings): Embedding(1, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): RobertaEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0-5): 6 x RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSdpaSelfAttention(\n",
      "                (query): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (key): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (value): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (lm_head): RobertaLMHead(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (decoder): Linear(in_features=768, out_features=52000, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from src.team5.models.custom_model import CustomChemBERTaModel\n",
    "\n",
    "MS_model = CustomChemBERTaModel(model, MAX_FRAGMENTS, MAX_SEQ_LENGTH)\n",
    "\n",
    "print(MS_model)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name} has shape {param.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IDwMMYCaYAZK"
   },
   "source": [
    "# LoRA config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "y0PF6-qGZ683"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 40,801,312 || all params: 124,304,192 || trainable%: 32.8238\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"key\", \"query\", \"value\" \"intermediate\"],  # they seem to drop off the \"key\" often?\n",
    "    modules_to_save=[\n",
    "        \"lm_head\"\n",
    "    ],  # change this to the name of the new modules at the end.\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(MS_model, peft_config)\n",
    "\n",
    "peft_model.print_trainable_parameters()  # check that it's training the right things"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-q-zpo0kaF0p"
   },
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lars/workspace/scratch_repository/.venv/lib/python3.12/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f43657bed67f4954b533e6405c1d45e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../logs/test_trainer\",\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=0.1,\n",
    "    report_to=None,\n",
    ")\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "I3YvQNVqcGyF"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_collator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AdamW\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[1;32m      4\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[0;32m----> 5\u001b[0m     train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39m\u001b[43mdata_collator\u001b[49m\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m AdamW(peft_model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-5\u001b[39m)\n\u001b[1;32m      9\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_collator' is not defined"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=16, shuffle=True, collate_fn=data_collator\n",
    ")\n",
    "optimizer = AdamW(peft_model.parameters(), lr=5e-5)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "peft_model.to(device)\n",
    "peft_model.train()\n",
    "\n",
    "for epoch in range(training_args.num_train_epochs):\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        supplementary_data = batch[\"supplementary_data\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = peft_model(\n",
    "            input_ids=input_ids, supplementary_data=supplementary_data, labels=labels\n",
    "        )\n",
    "        loss = outputs[1]\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "01jClKYXdZ-Z"
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0FDkmqV7dXNf"
   },
   "outputs": [],
   "source": [
    "def prepare_inference_input(smiles, precursor_mz):\n",
    "    inputs = tokenizer(\n",
    "        smiles,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    inputs[\"supplementary_data\"] = torch.tensor(\n",
    "        [supplementary_data], dtype=torch.float\n",
    "    ).to(device)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "peft_model.eval()\n",
    "\n",
    "# Example data\n",
    "smiles_example = \"CCO\"\n",
    "supplementary_data_example = 0  # TODO\n",
    "\n",
    "# Prepare input\n",
    "inputs = prepare_inference_input(smiles_example, supplementary_data_example)\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    outputs = peft_model(**inputs)\n",
    "    logits = outputs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2D8bfVV9keeM"
   },
   "source": [
    "# Choices that affect the whole architecture\n",
    "\n",
    "*   Format for the supplementary data\n",
    "*   Format for the label data\n",
    "*   The format of the output of the new model\n",
    "\n",
    "\n",
    "\n",
    "### More modular choices (that are important)\n",
    "\n",
    "\n",
    "*   Whether we have to predict compound_class at inference\n",
    "*   Include in_silico data?\n",
    "*   Architeture of the modified ChemBERTa model\n",
    "*   LoRA parameters\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
