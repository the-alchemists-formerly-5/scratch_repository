{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"../data/raw/enveda_library_subset_10percent.parquet\"\n",
    "BASE_MODEL = \"seyonec/ChemBERTa-zinc-base-v1\"\n",
    "MAX_FRAGMENTS = 512 # from anton, max number of mzs/intensities\n",
    "MAX_SEQ_LENGTH = 512 # base model max seq length\n",
    "SUPPLEMENTARY_DATA_DIM = 75\n",
    "ENABLE_PROFILING = False # If turned on, will profile the training\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 3\n",
    "## Set WANDB_API_KEY environment variable to enable logging to wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tYhe3fk-sblS"
   },
   "source": [
    "# Messing around with ChemBERTa for fun and for education\n",
    "\n",
    "The first half of this colab is just fun experiments trying to understand ChemBERTa and it's tokenizer better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 420,
     "referenced_widgets": [
      "09447f7d805a456a8d9df123079fef9b",
      "491c2c5348da4cebb59112f76ff21867",
      "fe9ba8c3b518443ba11fd1e700dd2c24",
      "017803aafcb84942bd04ae4b6014d713",
      "7bb64ec2bc31458fac09dafbafc66030",
      "48ba3adb18ee45b085dcfcc74b8e4d19",
      "90c266cbe71d46aaade3ab541889fe07",
      "ed10bed1b375495cb383786278b88953",
      "0186504a6df04889965a532aeaa3058c",
      "9365963655bf4a6f9f65418af1213afb",
      "e8e0cfd2025e41cfb5ea102586988b28",
      "6b54217022de4316a071ee63832b77fe",
      "3cf79467e3384aeba7e957596b912e29",
      "d23dd09659b9428f8cc8083f44e04709",
      "a75c8346a8f74f43941aa43700e6b2d8",
      "e5968c80140b4a71978dbb453ee75af2",
      "285f3c7db7d448a7a9ed62b2f9324e53",
      "e02702dc96744f0bb3304606cfa29e24",
      "5810f7fa342f4543834d8274abe552f5",
      "433bfc6fb35a4199b1cedfbaf5f5a15e",
      "8d37ac015a0f4caebf5455c7c16284dc",
      "f3084e63613741098216632a5f92ef98",
      "1b99d732290a4190982895ff640ea719",
      "b284f3ac0854497fbab925eeb1ded0df",
      "01638020753146678ee0a7692603b6c2",
      "6cd3e39a24fa4ed490ea0567c51c6eee",
      "e0c994de1d654e92ac91d661932ebdfa",
      "2fa221bf24cb4bdf9695b5f593b0d818",
      "502de19d537d41a7b151ca5798f46a72",
      "b689e92305ae4b6693991b37e894f4a0",
      "50e84fe3afd94297a3f198d0dfc4654b",
      "dcc379d6713f47529ea18e30482cfa29",
      "6d6719e9bdcc411c95f60201ee37119a",
      "ec2a4d26292d4606bdcee5c2a0ea5f70",
      "cf7c0c5e780145cf90de13cd375c1cce",
      "6cc566ec625c4251908f083a4ac5e972",
      "ea2e9aa05204487fa884178986a59380",
      "6ff00156a8d1407e8a03899aede36f60",
      "57196d402589425baba759c1f92b3fee",
      "931340e0216b4cdd940a7041f14b62f1",
      "ccba3a5e8f3f4f08b79e5cad740b78b8",
      "f762dd67830c45a1b8725313b92346e4",
      "bc455b59103e4404b9f9b124d6999b3c",
      "d178a1d70c32435482c10d6824bf4131",
      "7de2b548546d4c58b4e1af83cdf618e7",
      "b5c94d24b105493796b8f70d4adb55a7",
      "be8b9e625b624ad89abb785ffdb352b9",
      "7f60ccd803d8490f990a58518b1aa4dd",
      "038ff99e2ffc4f7db7ecd54333a3d71e",
      "177f0b132f2f40b9b63a048635f0dc70",
      "47da5e41d88d4bc6bfb496adacddba30",
      "604c8bed2d6f4c04bb9ae2b78c6f4aea",
      "c83d02b54fe94d6889aae124641f43eb",
      "4df1d8407ea74a3193426b27a15eeaa9",
      "67e3bc2be1034a73be1fc0b3b97da027",
      "f53b7a64750849d18bb4df54237a1110",
      "f7b208a676664b59a90d7d38293df4dd",
      "504f51d204af44b7915060072ea359b2",
      "1f2793e435b04dc7b9502f643a98d533",
      "c9e22b771e9e49ad8354902723d9b951",
      "201c3ab1ff3a46128961e12e4edb2a01",
      "bd46618a5bce46ec840bae0adb001564",
      "ac00b65c79fc4b73a3e2d9d437001e3f",
      "d9c52550dcb8430a891ada649a35d5de",
      "4a99e037f63b4dbfa9d2f58912c46167",
      "f89668c6b88c4bad853403c95eed4fff"
     ]
    },
    "id": "KPjZH305wYrn",
    "outputId": "a36beb4d-b472-44c7-aed8-c200e1cc515c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at seyonec/ChemBERTa-zinc-base-v1 were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (AutoModelForMaskedLM, AutoTokenizer)\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForMaskedLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(BASE_MODEL)\n\u001b[0;32m----> 4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBASE_MODEL\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/scratch_repository/.venv/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:923\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    921\u001b[0m model_type \u001b[38;5;241m=\u001b[39m config_class_to_model_type(\u001b[38;5;28mtype\u001b[39m(config)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 923\u001b[0m     tokenizer_class_py, tokenizer_class_fast \u001b[38;5;241m=\u001b[39m \u001b[43mTOKENIZER_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    925\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_fast \u001b[38;5;129;01mand\u001b[39;00m (use_fast \u001b[38;5;129;01mor\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class_fast\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/workspace/scratch_repository/.venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:763\u001b[0m, in \u001b[0;36m_LazyAutoMapping.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_type \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping:\n\u001b[1;32m    762\u001b[0m     model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping[model_type]\n\u001b[0;32m--> 763\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_attr_from_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    765\u001b[0m \u001b[38;5;66;03m# Maybe there was several model types associated with this config.\u001b[39;00m\n\u001b[1;32m    766\u001b[0m model_types \u001b[38;5;241m=\u001b[39m [k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config_mapping\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;241m==\u001b[39m key\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m]\n",
      "File \u001b[0;32m~/workspace/scratch_repository/.venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:777\u001b[0m, in \u001b[0;36m_LazyAutoMapping._load_attr_from_module\u001b[0;34m(self, model_type, attr)\u001b[0m\n\u001b[1;32m    775\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m module_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n\u001b[1;32m    776\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules[module_name] \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformers.models\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 777\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgetattribute_from_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_modules\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/scratch_repository/.venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:692\u001b[0m, in \u001b[0;36mgetattribute_from_module\u001b[0;34m(module, attr)\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(attr, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgetattribute_from_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(module, attr):\n\u001b[1;32m    694\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, attr)\n",
      "File \u001b[0;32m~/workspace/scratch_repository/.venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:692\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(attr, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[43mgetattribute_from_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m attr)\n\u001b[1;32m    693\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(module, attr):\n\u001b[1;32m    694\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, attr)\n",
      "File \u001b[0;32m~/workspace/scratch_repository/.venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:693\u001b[0m, in \u001b[0;36mgetattribute_from_module\u001b[0;34m(module, attr)\u001b[0m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(attr, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(getattribute_from_module(module, a) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m attr)\n\u001b[0;32m--> 693\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mhasattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    694\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, attr)\n\u001b[1;32m    695\u001b[0m \u001b[38;5;66;03m# Some of the mappings have entries model_type -> object of another model type. In that case we try to grab the\u001b[39;00m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;66;03m# object at the top level.\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/scratch_repository/.venv/lib/python3.12/site-packages/transformers/utils/import_utils.py:1754\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1752\u001b[0m     value \u001b[38;5;241m=\u001b[39m Placeholder\n\u001b[1;32m   1753\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m-> 1754\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1755\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1756\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/workspace/scratch_repository/.venv/lib/python3.12/site-packages/transformers/utils/import_utils.py:1764\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1764\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1765\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1766\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1767\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1768\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1769\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.7_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/importlib/__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:995\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:488\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/workspace/scratch_repository/.venv/lib/python3.12/site-packages/transformers/models/roberta/tokenization_roberta.py:22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lru_cache\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List, Optional, Tuple\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mregex\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenization_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AddedToken, PreTrainedTokenizer\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logging\n",
      "File \u001b[0;32m~/workspace/scratch_repository/.venv/lib/python3.12/site-packages/regex/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregex\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m regex\n\u001b[1;32m      3\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m regex\u001b[38;5;241m.\u001b[39m__all__\n",
      "File \u001b[0;32m~/workspace/scratch_repository/.venv/lib/python3.12/site-packages/regex/regex.py:417\u001b[0m\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m r\n\u001b[1;32m    414\u001b[0m \u001b[38;5;66;03m# --------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;66;03m# Internals.\u001b[39;00m\n\u001b[0;32m--> 417\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mregex\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_regex_core\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_regex_core\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mregex\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_regex\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_regex\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthreading\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RLock \u001b[38;5;28;01mas\u001b[39;00m _RLock\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:991\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1124\u001b[0m, in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:753\u001b[0m, in \u001b[0;36m_compile_bytecode\u001b[0;34m(data, name, bytecode_path, source_path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import (AutoModelForMaskedLM, AutoTokenizer)\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(BASE_MODEL)\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_cMXHTFeMOy"
   },
   "source": [
    "# Data prep\n",
    "\n",
    "\n",
    "These are the columns in the data set:\n",
    "\n",
    "precursor_mz - f64\n",
    "precursor_charge - f64\n",
    "mzs - list[f64]\n",
    "intensities - list[f64]\n",
    "in_silico - bool\n",
    "smiles - str\n",
    "adduct - str\n",
    "collision_energy - str\n",
    "instrument_type - str\n",
    "compound_class - str\n",
    "entropy - f64\n",
    "scaffold_smiles - str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "nyTWxIYveLsl",
    "outputId": "b83c7ecc-b236-47ff-9062-4dcb1e3db1f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['precursor_mz', 'precursor_charge', 'mzs', 'intensities', 'in_silico', 'smiles', 'adduct', 'collision_energy', 'instrument_type', 'compound_class', 'entropy', 'scaffold_smiles']\n",
      "['precursor_mz', 'precursor_charge', 'mzs', 'intensities', 'in_silico', 'smiles', 'adduct', 'collision_energy', 'instrument_type', 'compound_class', 'entropy', 'scaffold_smiles']\n",
      "shape: (5, 12)\n",
      "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬──────────┬───────────┐\n",
      "│ precursor ┆ precursor ┆ mzs       ┆ intensiti ┆ … ┆ instrumen ┆ compound_ ┆ entropy  ┆ scaffold_ │\n",
      "│ _mz       ┆ _charge   ┆ ---       ┆ es        ┆   ┆ t_type    ┆ class     ┆ ---      ┆ smiles    │\n",
      "│ ---       ┆ ---       ┆ list[f64] ┆ ---       ┆   ┆ ---       ┆ ---       ┆ f64      ┆ ---       │\n",
      "│ f64       ┆ f64       ┆           ┆ list[f64] ┆   ┆ str       ┆ str       ┆          ┆ str       │\n",
      "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪══════════╪═══════════╡\n",
      "│ 657.54634 ┆ -1.0      ┆ [41.00329 ┆ [0.002206 ┆ … ┆ cfm-predi ┆ Triacylgl ┆ 4.160459 ┆           │\n",
      "│ 9         ┆           ┆ ,         ┆ ,         ┆   ┆ ct 4      ┆ ycerols   ┆          ┆           │\n",
      "│           ┆           ┆ 43.01894, ┆ 0.14623,  ┆   ┆           ┆           ┆          ┆           │\n",
      "│           ┆           ┆ …         ┆ …         ┆   ┆           ┆           ┆          ┆           │\n",
      "│           ┆           ┆ 657.546…  ┆ 0.588434… ┆   ┆           ┆           ┆          ┆           │\n",
      "│ 827.71231 ┆ 1.0       ┆ [43.05423 ┆ [0.122531 ┆ … ┆ cfm-predi ┆ Triacylgl ┆ 4.856278 ┆           │\n",
      "│ 7         ┆           ┆ ,         ┆ ,         ┆   ┆ ct 4      ┆ ycerols   ┆          ┆           │\n",
      "│           ┆           ┆ 45.06988, ┆ 0.013169, ┆   ┆           ┆           ┆          ┆           │\n",
      "│           ┆           ┆ …         ┆ …         ┆   ┆           ┆           ┆          ┆           │\n",
      "│           ┆           ┆ 827.712…  ┆ 0.89753…  ┆   ┆           ┆           ┆          ┆           │\n",
      "│ 949.72796 ┆ 1.0       ┆ [39.02293 ┆ [0.012077 ┆ … ┆ cfm-predi ┆ Triacylgl ┆ 5.097924 ┆           │\n",
      "│ 7         ┆           ┆ ,         ┆ ,         ┆   ┆ ct 4      ┆ ycerols   ┆          ┆           │\n",
      "│           ┆           ┆ 41.03858, ┆ 0.064455, ┆   ┆           ┆           ┆          ┆           │\n",
      "│           ┆           ┆ …         ┆ …         ┆   ┆           ┆           ┆          ┆           │\n",
      "│           ┆           ┆ 949.727…  ┆ 0.86479…  ┆   ┆           ┆           ┆          ┆           │\n",
      "│ 925.85935 ┆ -1.0      ┆ [44.9982, ┆ [0.027488 ┆ … ┆ cfm-predi ┆ Triacylgl ┆ 4.282513 ┆           │\n",
      "│ 1         ┆           ┆ 47.01385, ┆ ,         ┆   ┆ ct 4      ┆ ycerols   ┆          ┆           │\n",
      "│           ┆           ┆ …         ┆ 0.006652, ┆   ┆           ┆           ┆          ┆           │\n",
      "│           ┆           ┆ 925.8593… ┆ …         ┆   ┆           ┆           ┆          ┆           │\n",
      "│           ┆           ┆           ┆ 0.60526…  ┆   ┆           ┆           ┆          ┆           │\n",
      "│ 357.30103 ┆ -1.0      ┆ [41.00329 ┆ [0.072312 ┆ … ┆ cfm-predi ┆ Monoacylg ┆ 3.550469 ┆           │\n",
      "│ 4         ┆           ┆ ,         ┆ ,         ┆   ┆ ct 4      ┆ lycerols  ┆          ┆           │\n",
      "│           ┆           ┆ 43.01894, ┆ 0.146965, ┆   ┆           ┆           ┆          ┆           │\n",
      "│           ┆           ┆ …         ┆ …         ┆   ┆           ┆           ┆          ┆           │\n",
      "│           ┆           ┆ 357.301…  ┆ 0.25859…  ┆   ┆           ┆           ┆          ┆           │\n",
      "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴──────────┴───────────┘\n",
      "shape: (5, 12)\n",
      "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬──────────┬───────────┐\n",
      "│ precursor ┆ precursor ┆ mzs       ┆ intensiti ┆ … ┆ instrumen ┆ compound_ ┆ entropy  ┆ scaffold_ │\n",
      "│ _mz       ┆ _charge   ┆ ---       ┆ es        ┆   ┆ t_type    ┆ class     ┆ ---      ┆ smiles    │\n",
      "│ ---       ┆ ---       ┆ list[f64] ┆ ---       ┆   ┆ ---       ┆ ---       ┆ f64      ┆ ---       │\n",
      "│ f64       ┆ f64       ┆           ┆ list[f64] ┆   ┆ str       ┆ str       ┆          ┆ str       │\n",
      "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪══════════╪═══════════╡\n",
      "│ 527.13365 ┆ 1.0       ┆ [41.00219 ┆ [0.010587 ┆ … ┆ cfm-predi ┆ Chalcones ┆ 3.58425  ┆ O=C(CC(c1 │\n",
      "│ 8         ┆           ┆ ,         ┆ ,         ┆   ┆ ct 4      ┆           ┆          ┆ ccccc1)c1 │\n",
      "│           ┆           ┆ 43.01784, ┆ 0.023416, ┆   ┆           ┆           ┆          ┆ c(-c2cccc │\n",
      "│           ┆           ┆ …         ┆ …         ┆   ┆           ┆           ┆          ┆ c2)…      │\n",
      "│           ┆           ┆ 527.133…  ┆ 0.87760…  ┆   ┆           ┆           ┆          ┆           │\n",
      "│ 866.46180 ┆ 1.0       ┆ [41.03858 ┆ [0.15955, ┆ … ┆ cfm-predi ┆ Cyclic    ┆ 4.035891 ┆ O=C(CC12C │\n",
      "│ 9         ┆           ┆ ,         ┆ 0.176604, ┆   ┆ ct 4      ┆ peptides  ┆          ┆ CC(CO1)CO │\n",
      "│           ┆           ┆ 42.03383, ┆ … 1.0]    ┆   ┆           ┆           ┆          ┆ 2)NC1COC( │\n",
      "│           ┆           ┆ …         ┆           ┆   ┆           ┆           ┆          ┆ =O)…      │\n",
      "│           ┆           ┆ 866.461…  ┆           ┆   ┆           ┆           ┆          ┆           │\n",
      "│ 325.04844 ┆ -1.0      ┆ [34.9694, ┆ [0.335189 ┆ … ┆ cfm-predi ┆ Meroterpe ┆ 4.01902  ┆ O=C(CC12C │\n",
      "│           ┆           ┆ 41.00329, ┆ ,         ┆   ┆ ct 4      ┆ noids     ┆          ┆ CCC1COC2= │\n",
      "│           ┆           ┆ …         ┆ 0.514464, ┆   ┆           ┆ with      ┆          ┆ O)c1ccccc │\n",
      "│           ┆           ┆ 325.0484… ┆ … 1.0]    ┆   ┆           ┆ bridged   ┆          ┆ 1         │\n",
      "│           ┆           ┆           ┆           ┆   ┆           ┆ ri…       ┆          ┆           │\n",
      "│ 583.33777 ┆ 1.0       ┆ [30.03383 ┆ [0.009245 ┆ … ┆ cfm-predi ┆ Terpenoid ┆ 4.019998 ┆ O=C(CC12C │\n",
      "│ 8         ┆           ┆ ,         ┆ ,         ┆   ┆ ct 4      ┆ alkaloids ┆          ┆ CCC34C5CC │\n",
      "│           ┆           ┆ 32.04948, ┆ 0.007699, ┆   ┆           ┆           ┆          ┆ 6CCC(C5C6 │\n",
      "│           ┆           ┆ …         ┆ … 1.0]    ┆   ┆           ┆           ┆          ┆ )C(…      │\n",
      "│           ┆           ┆ 583.337…  ┆           ┆   ┆           ┆           ┆          ┆           │\n",
      "│ 679.40628 ┆ -1.0      ┆ [41.00329 ┆ [0.074812 ┆ … ┆ cfm-predi ┆ Ursane    ┆ 3.841568 ┆ O=C(CC12C │\n",
      "│ 7         ┆           ┆ ,         ┆ ,         ┆   ┆ ct 4      ┆ and Tarax ┆          ┆ CCCC1C1=C │\n",
      "│           ┆           ┆ 43.01894, ┆ 0.090091, ┆   ┆           ┆ astane    ┆          ┆ CC3C4CCCC │\n",
      "│           ┆           ┆ …         ┆ … 1.0]    ┆   ┆           ┆ triterp…  ┆          ┆ C4C…      │\n",
      "│           ┆           ┆ 679.406…  ┆           ┆   ┆           ┆           ┆          ┆           │\n",
      "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴──────────┴───────────┘\n",
      "Loading tokenizer\n",
      "Transforming raw data and writing prepared data to disk\n",
      "Reading prepared data from disk\n",
      "Loading tokenizer\n",
      "Transforming raw data and writing prepared data to disk\n",
      "Reading prepared data from disk\n"
     ]
    }
   ],
   "source": [
    "# import the data (with pandas?)\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "## Load the dataset (for some reason this didn't work for me)\n",
    "# df = pd.read_parquet('enveda_library_subset 2.parquet')\n",
    "\n",
    "# print(df.head())\n",
    "\n",
    "\n",
    "# custom Dataset class for all the types of data.\n",
    "# I think we might want to make a new 'column' of data that combines mzs and intensities into \"label\"\n",
    "\n",
    "from src.team5.data.data_loader import SMILESDataset\n",
    "from src.team5.data.data_split import sort_dataframe_by_scaffold, split_dataframe\n",
    "from src.team5.data.prepare import tensorize\n",
    "\n",
    "df = pl.read_parquet(DATASET)\n",
    "\n",
    "df_sorted = sort_dataframe_by_scaffold(df)\n",
    "\n",
    "df_train, df_test = split_dataframe(df_sorted, split_ratio=0.9)\n",
    "\n",
    "# Print column names\n",
    "print(df_train.columns)\n",
    "print(df_test.columns)\n",
    "print(df_train.head())\n",
    "print(df_test.head())\n",
    "\n",
    "(train_tokenized_smiles, train_attention_mask, train_labels, train_supplementary_data) = tensorize(df_train)\n",
    "(test_tokenized_smiles, test_attention_mask, test_labels, test_supplementary_data) = tensorize(df_test)\n",
    "\n",
    "train_dataset = SMILESDataset(train_tokenized_smiles, train_attention_mask, train_labels, train_supplementary_data)\n",
    "test_dataset = SMILESDataset(test_tokenized_smiles, test_attention_mask, test_labels, test_supplementary_data)\n",
    "\n",
    "## batch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': torch.int64, 'attention_mask': torch.int64, 'labels': torch.float32, 'supplementary_data': torch.float32}\n"
     ]
    }
   ],
   "source": [
    "print({k:v.dtype for k,v in train_dataset[0].items()})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fENovYIHeaOO"
   },
   "source": [
    "# Custom model for our problem\n",
    "This is probably the most important part in terms of design choices. We are changing the ChemBERTa model by adding on something at the end. This new module will take the hidden SMILES embedding from the last hidden layer as input. It will also take in all the other data about the precusor molecule and experimental conditions (eg, precusor mz, collison energy etc). For now, let's call that supplementary data.\n",
    "\n",
    "I've written the simplest possible thing here: a single linear layer that takes the embedding of the entire seq, concatinated with all the supplementary data for the example. It outputs \"labels\", which is mzs and intensities zipped together.\n",
    "\n",
    "The reason for making a single module output both mzs and intensities is because there needs to be the same number of fragments per example, and the two numbers are very related.\n",
    "\n",
    "A single linear layer is probably a terrible choice though, since this is the only layer that sees all the supplementary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "rMogXqZteZ1Z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomChemBERTaModel(\n",
      "  (model): RobertaForMaskedLM(\n",
      "    (roberta): RobertaModel(\n",
      "      (embeddings): RobertaEmbeddings(\n",
      "        (word_embeddings): Embedding(767, 768, padding_idx=1)\n",
      "        (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "        (token_type_embeddings): Embedding(1, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): RobertaEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0-5): 6 x RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSdpaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (lm_head): RobertaLMHead(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (decoder): Linear(in_features=768, out_features=767, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (final_layers): FinalLayers(\n",
      "    (layer1): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (activation1): ReLU()\n",
      "    (layer2): Linear(in_features=843, out_features=16, bias=True)\n",
      "    (layer3): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "  )\n",
      ")\n",
      "model.roberta.embeddings.word_embeddings.weight has shape torch.Size([767, 768])\n",
      "model.roberta.embeddings.position_embeddings.weight has shape torch.Size([514, 768])\n",
      "model.roberta.embeddings.token_type_embeddings.weight has shape torch.Size([1, 768])\n",
      "model.roberta.embeddings.LayerNorm.weight has shape torch.Size([768])\n",
      "model.roberta.embeddings.LayerNorm.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.0.attention.self.query.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.0.attention.self.query.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.0.attention.self.key.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.0.attention.self.key.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.0.attention.self.value.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.0.attention.self.value.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.0.attention.output.dense.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.0.attention.output.dense.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.0.attention.output.LayerNorm.weight has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.0.attention.output.LayerNorm.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.0.intermediate.dense.weight has shape torch.Size([3072, 768])\n",
      "model.roberta.encoder.layer.0.intermediate.dense.bias has shape torch.Size([3072])\n",
      "model.roberta.encoder.layer.0.output.dense.weight has shape torch.Size([768, 3072])\n",
      "model.roberta.encoder.layer.0.output.dense.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.0.output.LayerNorm.weight has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.0.output.LayerNorm.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.1.attention.self.query.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.1.attention.self.query.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.1.attention.self.key.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.1.attention.self.key.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.1.attention.self.value.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.1.attention.self.value.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.1.attention.output.dense.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.1.attention.output.dense.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.1.attention.output.LayerNorm.weight has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.1.attention.output.LayerNorm.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.1.intermediate.dense.weight has shape torch.Size([3072, 768])\n",
      "model.roberta.encoder.layer.1.intermediate.dense.bias has shape torch.Size([3072])\n",
      "model.roberta.encoder.layer.1.output.dense.weight has shape torch.Size([768, 3072])\n",
      "model.roberta.encoder.layer.1.output.dense.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.1.output.LayerNorm.weight has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.1.output.LayerNorm.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.2.attention.self.query.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.2.attention.self.query.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.2.attention.self.key.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.2.attention.self.key.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.2.attention.self.value.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.2.attention.self.value.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.2.attention.output.dense.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.2.attention.output.dense.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.2.attention.output.LayerNorm.weight has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.2.attention.output.LayerNorm.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.2.intermediate.dense.weight has shape torch.Size([3072, 768])\n",
      "model.roberta.encoder.layer.2.intermediate.dense.bias has shape torch.Size([3072])\n",
      "model.roberta.encoder.layer.2.output.dense.weight has shape torch.Size([768, 3072])\n",
      "model.roberta.encoder.layer.2.output.dense.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.2.output.LayerNorm.weight has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.2.output.LayerNorm.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.3.attention.self.query.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.3.attention.self.query.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.3.attention.self.key.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.3.attention.self.key.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.3.attention.self.value.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.3.attention.self.value.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.3.attention.output.dense.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.3.attention.output.dense.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.3.attention.output.LayerNorm.weight has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.3.attention.output.LayerNorm.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.3.intermediate.dense.weight has shape torch.Size([3072, 768])\n",
      "model.roberta.encoder.layer.3.intermediate.dense.bias has shape torch.Size([3072])\n",
      "model.roberta.encoder.layer.3.output.dense.weight has shape torch.Size([768, 3072])\n",
      "model.roberta.encoder.layer.3.output.dense.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.3.output.LayerNorm.weight has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.3.output.LayerNorm.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.4.attention.self.query.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.4.attention.self.query.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.4.attention.self.key.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.4.attention.self.key.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.4.attention.self.value.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.4.attention.self.value.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.4.attention.output.dense.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.4.attention.output.dense.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.4.attention.output.LayerNorm.weight has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.4.attention.output.LayerNorm.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.4.intermediate.dense.weight has shape torch.Size([3072, 768])\n",
      "model.roberta.encoder.layer.4.intermediate.dense.bias has shape torch.Size([3072])\n",
      "model.roberta.encoder.layer.4.output.dense.weight has shape torch.Size([768, 3072])\n",
      "model.roberta.encoder.layer.4.output.dense.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.4.output.LayerNorm.weight has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.4.output.LayerNorm.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.5.attention.self.query.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.5.attention.self.query.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.5.attention.self.key.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.5.attention.self.key.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.5.attention.self.value.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.5.attention.self.value.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.5.attention.output.dense.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.5.attention.output.dense.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.5.attention.output.LayerNorm.weight has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.5.attention.output.LayerNorm.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.5.intermediate.dense.weight has shape torch.Size([3072, 768])\n",
      "model.roberta.encoder.layer.5.intermediate.dense.bias has shape torch.Size([3072])\n",
      "model.roberta.encoder.layer.5.output.dense.weight has shape torch.Size([768, 3072])\n",
      "model.roberta.encoder.layer.5.output.dense.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.5.output.LayerNorm.weight has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.5.output.LayerNorm.bias has shape torch.Size([768])\n",
      "model.lm_head.bias has shape torch.Size([767])\n",
      "model.lm_head.dense.weight has shape torch.Size([768, 768])\n",
      "model.lm_head.dense.bias has shape torch.Size([768])\n",
      "model.lm_head.layer_norm.weight has shape torch.Size([768])\n",
      "model.lm_head.layer_norm.bias has shape torch.Size([768])\n",
      "final_layers.layer1.weight has shape torch.Size([128, 512])\n",
      "final_layers.layer1.bias has shape torch.Size([128])\n",
      "final_layers.layer2.weight has shape torch.Size([16, 843])\n",
      "final_layers.layer2.bias has shape torch.Size([16])\n",
      "final_layers.layer3.weight has shape torch.Size([1024, 2048])\n",
      "final_layers.layer3.bias has shape torch.Size([1024])\n"
     ]
    }
   ],
   "source": [
    "from src.team5.models.custom_model import CustomChemBERTaModel\n",
    "\n",
    "MS_model = CustomChemBERTaModel(model, MAX_FRAGMENTS, MAX_SEQ_LENGTH, SUPPLEMENTARY_DATA_DIM)\n",
    "\n",
    "print(MS_model)\n",
    "\n",
    "for name, param in MS_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name} has shape {param.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IDwMMYCaYAZK"
   },
   "source": [
    "# LoRA config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "y0PF6-qGZ683"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,324,800 || all params: 48,608,383 || trainable%: 4.7827\n",
      "base_model.model.model.roberta.encoder.layer.0.attention.self.query.lora_A.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.0.attention.self.query.lora_B.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.0.attention.self.value.lora_A.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.0.attention.self.value.lora_B.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.1.attention.self.query.lora_A.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.1.attention.self.query.lora_B.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.1.attention.self.value.lora_A.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.1.attention.self.value.lora_B.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.2.attention.self.query.lora_A.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.2.attention.self.query.lora_B.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.2.attention.self.value.lora_A.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.2.attention.self.value.lora_B.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.3.attention.self.query.lora_A.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.3.attention.self.query.lora_B.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.3.attention.self.value.lora_A.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.3.attention.self.value.lora_B.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.4.attention.self.query.lora_A.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.4.attention.self.query.lora_B.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.4.attention.self.value.lora_A.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.4.attention.self.value.lora_B.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.5.attention.self.query.lora_A.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.5.attention.self.query.lora_B.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.5.attention.self.value.lora_A.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.5.attention.self.value.lora_B.default.weight is trainable\n",
      "base_model.model.final_layers.modules_to_save.default.layer1.weight is trainable\n",
      "base_model.model.final_layers.modules_to_save.default.layer1.bias is trainable\n",
      "base_model.model.final_layers.modules_to_save.default.layer2.weight is trainable\n",
      "base_model.model.final_layers.modules_to_save.default.layer2.bias is trainable\n",
      "base_model.model.final_layers.modules_to_save.default.layer3.weight is trainable\n",
      "base_model.model.final_layers.modules_to_save.default.layer3.bias is trainable\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"query\", \"value\"],\n",
    "    modules_to_save=[\n",
    "        \"final_layers\"\n",
    "    ],  # change this to the name of the new modules at the end.\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(MS_model, peft_config)\n",
    "\n",
    "peft_model.print_trainable_parameters()  # check that it's training the right things\n",
    "for name, param in peft_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name} is trainable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-q-zpo0kaF0p"
   },
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import torch.nn as nn\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "class ProfilingCallback(TrainerCallback):\n",
    "    def __init__(self, device, n_steps=10):\n",
    "        self.device = device\n",
    "        self.n_steps = n_steps\n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if state.global_step % self.n_steps == 0:\n",
    "            with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "                         profile_memory=True, record_shapes=True) as prof:\n",
    "                with record_function(\"model_inference\"):\n",
    "                    # Run a forward pass\n",
    "                    example = train_dataset[0]\n",
    "                    # Each field in the example is a tensor, so we need to add a batch dimension to the front of each\n",
    "                    example = {k: v.unsqueeze(0).to(self.device) for k, v in example.items()}\n",
    "                    peft_model(**example)\n",
    "            \n",
    "            print(f\"Step {state.global_step}\")\n",
    "            print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=20))\n",
    "            print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=20))\n",
    "            print(prof.key_averages().table(sort_by=\"cpu_memory_usage\", row_limit=20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "base_model.model.model.roberta.encoder.layer.0.attention.self.query.lora_A.default.weight is placed on mps:0\n",
      "base_model.model.model.roberta.encoder.layer.0.attention.self.query.lora_B.default.weight is placed on mps:0\n",
      "base_model.model.model.roberta.encoder.layer.0.attention.self.value.lora_A.default.weight is placed on mps:0\n",
      "base_model.model.model.roberta.encoder.layer.0.attention.self.value.lora_B.default.weight is placed on mps:0\n",
      "base_model.model.model.roberta.encoder.layer.1.attention.self.query.lora_A.default.weight is placed on mps:0\n",
      "base_model.model.model.roberta.encoder.layer.1.attention.self.query.lora_B.default.weight is placed on mps:0\n",
      "base_model.model.model.roberta.encoder.layer.1.attention.self.value.lora_A.default.weight is placed on mps:0\n",
      "base_model.model.model.roberta.encoder.layer.1.attention.self.value.lora_B.default.weight is placed on mps:0\n",
      "base_model.model.model.roberta.encoder.layer.2.attention.self.query.lora_A.default.weight is placed on mps:0\n",
      "base_model.model.model.roberta.encoder.layer.2.attention.self.query.lora_B.default.weight is placed on mps:0\n",
      "base_model.model.model.roberta.encoder.layer.2.attention.self.value.lora_A.default.weight is placed on mps:0\n",
      "base_model.model.model.roberta.encoder.layer.2.attention.self.value.lora_B.default.weight is placed on mps:0\n",
      "base_model.model.model.roberta.encoder.layer.3.attention.self.query.lora_A.default.weight is placed on mps:0\n",
      "base_model.model.model.roberta.encoder.layer.3.attention.self.query.lora_B.default.weight is placed on mps:0\n",
      "base_model.model.model.roberta.encoder.layer.3.attention.self.value.lora_A.default.weight is placed on mps:0\n",
      "base_model.model.model.roberta.encoder.layer.3.attention.self.value.lora_B.default.weight is placed on mps:0\n",
      "base_model.model.model.roberta.encoder.layer.4.attention.self.query.lora_A.default.weight is placed on mps:0\n",
      "base_model.model.model.roberta.encoder.layer.4.attention.self.query.lora_B.default.weight is placed on mps:0\n",
      "base_model.model.model.roberta.encoder.layer.4.attention.self.value.lora_A.default.weight is placed on mps:0\n",
      "base_model.model.model.roberta.encoder.layer.4.attention.self.value.lora_B.default.weight is placed on mps:0\n",
      "base_model.model.model.roberta.encoder.layer.5.attention.self.query.lora_A.default.weight is placed on mps:0\n",
      "base_model.model.model.roberta.encoder.layer.5.attention.self.query.lora_B.default.weight is placed on mps:0\n",
      "base_model.model.model.roberta.encoder.layer.5.attention.self.value.lora_A.default.weight is placed on mps:0\n",
      "base_model.model.model.roberta.encoder.layer.5.attention.self.value.lora_B.default.weight is placed on mps:0\n",
      "base_model.model.final_layers.modules_to_save.default.layer1.weight is placed on mps:0\n",
      "base_model.model.final_layers.modules_to_save.default.layer1.bias is placed on mps:0\n",
      "base_model.model.final_layers.modules_to_save.default.layer2.weight is placed on mps:0\n",
      "base_model.model.final_layers.modules_to_save.default.layer2.bias is placed on mps:0\n",
      "base_model.model.final_layers.modules_to_save.default.layer3.weight is placed on mps:0\n",
      "base_model.model.final_layers.modules_to_save.default.layer3.bias is placed on mps:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lars/workspace/scratch_repository/.venv/lib/python3.12/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/Users/lars/workspace/scratch_repository/.venv/lib/python3.12/site-packages/transformers/training_args.py:2199: UserWarning: `use_mps_device` is deprecated and will be removed in version 5.0 of 🤗 Transformers. `mps` device will be used by default if available similar to the way `cuda` device is used.Therefore, no action from user is required. \n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mteam5-hackathonbio\u001b[0m (\u001b[33mteam5-hackathon-bio\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/lars/workspace/scratch_repository/notebooks/wandb/run-20241013_120205-fcash9kw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/team5-hackathon-bio/hackathon/runs/fcash9kw' target=\"_blank\">../logs/training_2024-10-13</a></strong> to <a href='https://wandb.ai/team5-hackathon-bio/hackathon' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/team5-hackathon-bio/hackathon' target=\"_blank\">https://wandb.ai/team5-hackathon-bio/hackathon</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/team5-hackathon-bio/hackathon/runs/fcash9kw' target=\"_blank\">https://wandb.ai/team5-hackathon-bio/hackathon/runs/fcash9kw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63df40a363204430bf176b8dc732aab2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8766 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lars/workspace/scratch_repository/.venv/lib/python3.12/site-packages/torch/autograd/profiler.py:257: UserWarning: CUDA is not available, disabling CUDA profiling\n",
      "  warn(\"CUDA is not available, disabling CUDA profiling\")\n",
      "/Users/lars/workspace/scratch_repository/.venv/lib/python3.12/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1, 1024])) that is different to the input size (torch.Size([1024])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10\n",
      "--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                        Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
      "--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                             model_inference         0.20%       5.523ms       100.00%        2.763s        2.763s           0 b           0 b             1  \n",
      "                                aten::cumsum        37.13%        1.026s        37.13%        1.026s        1.026s           0 b           0 b             1  \n",
      "                                aten::linear        17.01%     470.016ms        17.01%     470.195ms       7.234ms           0 b           0 b            65  \n",
      "          aten::scaled_dot_product_attention         0.00%       8.916us        12.07%     333.470ms      55.578ms           0 b           0 b             6  \n",
      "    aten::_scaled_dot_product_attention_math         0.01%     244.379us        12.07%     333.461ms      55.577ms           0 b           0 b             6  \n",
      "                               aten::dropout         0.00%     115.132us        11.03%     304.899ms       9.835ms           0 b           0 b            31  \n",
      "                            aten::bernoulli_         7.50%     207.211ms         7.53%     208.205ms       6.716ms           0 b           0 b            31  \n",
      "                                   aten::mul         5.24%     144.817ms         5.25%     144.985ms       2.589ms           0 b           0 b            56  \n",
      "                                   aten::add         4.49%     124.088ms         4.49%     124.093ms       4.596ms           0 b           0 b            27  \n",
      "                                   aten::all         3.99%     110.153ms         3.99%     110.153ms     110.153ms           0 b           0 b             1  \n",
      "                              aten::mse_loss         3.78%     104.468ms         3.78%     104.470ms     104.470ms           0 b           0 b             1  \n",
      "                                 aten::copy_         3.33%      92.010ms         3.33%      92.018ms       3.286ms           0 b           0 b            28  \n",
      "                                    aten::to         0.00%      15.502us         3.33%      91.960ms       2.787ms           0 b           0 b            33  \n",
      "                              aten::_to_copy         0.00%      46.540us         3.33%      91.945ms      10.216ms           0 b           0 b             9  \n",
      "                               aten::softmax         0.01%     233.665us         3.05%      84.413ms      14.069ms           0 b           0 b             6  \n",
      "                              aten::_softmax         3.05%      84.179ms         3.05%      84.179ms      14.030ms           0 b           0 b             6  \n",
      "                                  aten::div_         1.69%      46.658ms         1.69%      46.684ms       1.506ms           0 b           0 b            31  \n",
      "                                    aten::ne         1.63%      45.150ms         1.63%      45.151ms      45.151ms           0 b           0 b             1  \n",
      "                           aten::masked_fill         0.00%       7.709us         1.57%      43.401ms      43.401ms           0 b           0 b             1  \n",
      "                          aten::masked_fill_         1.57%      43.366ms         1.57%      43.366ms      43.366ms           0 b           0 b             1  \n",
      "--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 2.763s\n",
      "\n",
      "--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                        Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
      "--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                             model_inference         0.20%       5.523ms       100.00%        2.763s        2.763s           0 b           0 b             1  \n",
      "                                aten::select         0.00%      15.832us         0.00%      17.708us       4.427us           0 b           0 b             4  \n",
      "                            aten::as_strided         0.00%      65.463us         0.00%      65.463us       0.818us           0 b           0 b            80  \n",
      "                             aten::unsqueeze         0.00%      23.624us         0.00%      29.375us       4.196us           0 b           0 b             7  \n",
      "                                    aten::to         0.00%      15.502us         3.33%      91.960ms       2.787ms           0 b           0 b            33  \n",
      "                              aten::_to_copy         0.00%      46.540us         3.33%      91.945ms      10.216ms           0 b           0 b             9  \n",
      "                         aten::empty_strided         0.00%      80.665us         0.00%      80.665us       2.521us           1 b           1 b            32  \n",
      "                                 aten::copy_         3.33%      92.010ms         3.33%      92.018ms       3.286ms           0 b           0 b            28  \n",
      "                             aten::expand_as         0.00%       2.751us         0.00%       7.793us       1.948us           0 b           0 b             4  \n",
      "                                aten::expand         0.00%      72.999us         0.00%      90.502us       2.828us           0 b           0 b            32  \n",
      "                                 aten::slice         0.00%      19.709us         0.00%      24.917us       6.229us           0 b           0 b             4  \n",
      "                                    aten::ne         1.63%      45.150ms         1.63%      45.151ms      45.151ms           0 b           0 b             1  \n",
      "                                  aten::item         0.00%      42.163us         1.04%      28.832ms     472.652us           0 b           0 b            61  \n",
      "                   aten::_local_scalar_dense         1.04%      28.780ms         1.04%      28.790ms     471.961us           0 b          -1 b            61  \n",
      "                                aten::cumsum        37.13%        1.026s        37.13%        1.026s        1.026s           0 b           0 b             1  \n",
      "                          aten::is_same_size         0.00%       5.373us         0.00%       5.373us       0.163us           0 b           0 b            33  \n",
      "                               aten::type_as         0.00%       3.125us         0.82%      22.774ms      22.774ms           0 b           0 b             1  \n",
      "                                   aten::add         4.49%     124.088ms         4.49%     124.093ms       4.596ms           0 b           0 b            27  \n",
      "                                   aten::mul         5.24%     144.817ms         5.25%     144.985ms       2.589ms           0 b           0 b            56  \n",
      "                             aten::embedding         0.00%      27.499us         0.28%       7.719ms       2.573ms           0 b           0 b             3  \n",
      "--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 2.763s\n",
      "\n",
      "--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                        Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
      "--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                         aten::empty_strided         0.00%      80.665us         0.00%      80.665us       2.521us           1 b           1 b            32  \n",
      "                            aten::empty_like         0.00%      65.334us         0.01%     239.496us       3.926us           1 b           0 b            61  \n",
      "                             model_inference         0.20%       5.523ms       100.00%        2.763s        2.763s           0 b           0 b             1  \n",
      "                                aten::select         0.00%      15.832us         0.00%      17.708us       4.427us           0 b           0 b             4  \n",
      "                            aten::as_strided         0.00%      65.463us         0.00%      65.463us       0.818us           0 b           0 b            80  \n",
      "                             aten::unsqueeze         0.00%      23.624us         0.00%      29.375us       4.196us           0 b           0 b             7  \n",
      "                                    aten::to         0.00%      15.502us         3.33%      91.960ms       2.787ms           0 b           0 b            33  \n",
      "                              aten::_to_copy         0.00%      46.540us         3.33%      91.945ms      10.216ms           0 b           0 b             9  \n",
      "                                 aten::copy_         3.33%      92.010ms         3.33%      92.018ms       3.286ms           0 b           0 b            28  \n",
      "                             aten::expand_as         0.00%       2.751us         0.00%       7.793us       1.948us           0 b           0 b             4  \n",
      "                                aten::expand         0.00%      72.999us         0.00%      90.502us       2.828us           0 b           0 b            32  \n",
      "                                 aten::slice         0.00%      19.709us         0.00%      24.917us       6.229us           0 b           0 b             4  \n",
      "                                    aten::ne         1.63%      45.150ms         1.63%      45.151ms      45.151ms           0 b           0 b             1  \n",
      "                                  aten::item         0.00%      42.163us         1.04%      28.832ms     472.652us           0 b           0 b            61  \n",
      "                   aten::_local_scalar_dense         1.04%      28.780ms         1.04%      28.790ms     471.961us           0 b          -1 b            61  \n",
      "                                aten::cumsum        37.13%        1.026s        37.13%        1.026s        1.026s           0 b           0 b             1  \n",
      "                          aten::is_same_size         0.00%       5.373us         0.00%       5.373us       0.163us           0 b           0 b            33  \n",
      "                               aten::type_as         0.00%       3.125us         0.82%      22.774ms      22.774ms           0 b           0 b             1  \n",
      "                                   aten::add         4.49%     124.088ms         4.49%     124.093ms       4.596ms           0 b           0 b            27  \n",
      "                                   aten::mul         5.24%     144.817ms         5.25%     144.985ms       2.589ms           0 b           0 b            56  \n",
      "--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 2.763s\n",
      "\n",
      "Step 20\n",
      "--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                        Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
      "--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                             model_inference        10.43%       1.650ms       100.00%      15.823ms      15.823ms           0 b           0 b             1  \n",
      "                               aten::dropout         0.43%      68.747us        24.64%       3.899ms     125.773us           0 b           0 b            31  \n",
      "                                aten::linear        15.03%       2.379ms        15.99%       2.529ms      38.914us           0 b           0 b            65  \n",
      "          aten::scaled_dot_product_attention         0.04%       5.956us        14.94%       2.364ms     394.037us           0 b           0 b             6  \n",
      "    aten::_scaled_dot_product_attention_math         0.27%      42.996us        14.90%       2.358ms     393.044us           0 b           0 b             6  \n",
      "                                   aten::mul        12.23%       1.936ms        12.75%       2.017ms      36.023us           0 b           0 b            56  \n",
      "                                 aten::copy_        11.07%       1.752ms        11.12%       1.760ms      62.857us           0 b           0 b            28  \n",
      "                                    aten::to         0.05%       7.293us        11.01%       1.741ms      52.771us           0 b           0 b            33  \n",
      "                            aten::bernoulli_         7.60%       1.202ms        10.98%       1.738ms      56.064us           0 b           0 b            31  \n",
      "                            aten::layer_norm         0.11%      17.376us        10.96%       1.735ms     123.921us           0 b           0 b            14  \n",
      "                              aten::_to_copy         0.09%      14.501us        10.96%       1.734ms     192.683us           0 b           0 b             9  \n",
      "                     aten::native_layer_norm         0.59%      92.912us        10.85%       1.718ms     122.680us           0 b           0 b            14  \n",
      "                     aten::native_batch_norm         5.96%     943.531us         6.28%     993.282us      70.949us           0 b           0 b            14  \n",
      "                                  aten::div_         5.99%     947.873us         6.07%     960.245us      30.976us           0 b           0 b            31  \n",
      "                                   aten::add         5.46%     864.491us         5.47%     865.492us      32.055us           0 b           0 b            27  \n",
      "                               aten::addcmul         3.71%     587.074us         3.84%     607.115us      43.365us           0 b           0 b            14  \n",
      "                                aten::matmul         0.27%      43.086us         3.45%     545.247us      45.437us           0 b           0 b            12  \n",
      "                                  aten::full         0.15%      24.163us         3.37%     533.531us      17.211us           0 b           0 b            31  \n",
      "                                 aten::fill_         3.10%     491.243us         3.10%     491.243us      15.847us           0 b           0 b            31  \n",
      "                                   aten::bmm         2.34%     369.496us         2.45%     387.579us      32.298us           0 b           0 b            12  \n",
      "--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 15.823ms\n",
      "\n",
      "--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                        Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
      "--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                             model_inference        10.43%       1.650ms       100.00%      15.823ms      15.823ms           0 b           0 b             1  \n",
      "                                aten::select         0.09%      13.459us         0.12%      18.708us       4.677us           0 b           0 b             4  \n",
      "                            aten::as_strided         0.42%      67.204us         0.42%      67.204us       0.840us           0 b           0 b            80  \n",
      "                             aten::unsqueeze         0.08%      13.332us         0.12%      19.665us       2.809us           0 b           0 b             7  \n",
      "                                    aten::to         0.05%       7.293us        11.01%       1.741ms      52.771us           0 b           0 b            33  \n",
      "                              aten::_to_copy         0.09%      14.501us        10.96%       1.734ms     192.683us           0 b           0 b             9  \n",
      "                         aten::empty_strided         0.27%      42.540us         0.27%      42.540us       1.329us           1 b           1 b            32  \n",
      "                                 aten::copy_        11.07%       1.752ms        11.12%       1.760ms      62.857us           0 b           0 b            28  \n",
      "                             aten::expand_as         0.02%       3.250us         0.05%       8.042us       2.010us           0 b           0 b             4  \n",
      "                                aten::expand         0.28%      45.045us         0.40%      62.625us       1.957us           0 b           0 b            32  \n",
      "                                 aten::slice         0.06%       9.167us         0.10%      15.792us       3.948us           0 b           0 b             4  \n",
      "                                    aten::ne         0.44%      68.916us         0.44%      70.124us      70.124us           0 b           0 b             1  \n",
      "                                  aten::item         0.14%      21.374us         2.00%     316.370us       5.186us           0 b           0 b            61  \n",
      "                   aten::_local_scalar_dense         1.86%     293.537us         1.86%     294.996us       4.836us           0 b          -1 b            61  \n",
      "                                aten::cumsum         0.38%      60.541us         0.38%      60.625us      60.625us           0 b           0 b             1  \n",
      "                          aten::is_same_size         0.02%       2.833us         0.02%       2.833us       0.086us           0 b           0 b            33  \n",
      "                               aten::type_as         0.00%       0.750us         0.16%      25.207us      25.207us           0 b           0 b             1  \n",
      "                                   aten::add         5.46%     864.491us         5.47%     865.492us      32.055us           0 b           0 b            27  \n",
      "                                   aten::mul        12.23%       1.936ms        12.75%       2.017ms      36.023us           0 b           0 b            56  \n",
      "                             aten::embedding         0.08%      12.289us         1.26%     198.622us      66.207us           0 b           0 b             3  \n",
      "--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 15.823ms\n",
      "\n",
      "--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                        Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
      "--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                         aten::empty_strided         0.27%      42.540us         0.27%      42.540us       1.329us           1 b           1 b            32  \n",
      "                            aten::empty_like         0.23%      35.825us         0.85%     134.246us       2.201us           1 b           0 b            61  \n",
      "                             model_inference        10.43%       1.650ms       100.00%      15.823ms      15.823ms           0 b           0 b             1  \n",
      "                                aten::select         0.09%      13.459us         0.12%      18.708us       4.677us           0 b           0 b             4  \n",
      "                            aten::as_strided         0.42%      67.204us         0.42%      67.204us       0.840us           0 b           0 b            80  \n",
      "                             aten::unsqueeze         0.08%      13.332us         0.12%      19.665us       2.809us           0 b           0 b             7  \n",
      "                                    aten::to         0.05%       7.293us        11.01%       1.741ms      52.771us           0 b           0 b            33  \n",
      "                              aten::_to_copy         0.09%      14.501us        10.96%       1.734ms     192.683us           0 b           0 b             9  \n",
      "                                 aten::copy_        11.07%       1.752ms        11.12%       1.760ms      62.857us           0 b           0 b            28  \n",
      "                             aten::expand_as         0.02%       3.250us         0.05%       8.042us       2.010us           0 b           0 b             4  \n",
      "                                aten::expand         0.28%      45.045us         0.40%      62.625us       1.957us           0 b           0 b            32  \n",
      "                                 aten::slice         0.06%       9.167us         0.10%      15.792us       3.948us           0 b           0 b             4  \n",
      "                                    aten::ne         0.44%      68.916us         0.44%      70.124us      70.124us           0 b           0 b             1  \n",
      "                                  aten::item         0.14%      21.374us         2.00%     316.370us       5.186us           0 b           0 b            61  \n",
      "                   aten::_local_scalar_dense         1.86%     293.537us         1.86%     294.996us       4.836us           0 b          -1 b            61  \n",
      "                                aten::cumsum         0.38%      60.541us         0.38%      60.625us      60.625us           0 b           0 b             1  \n",
      "                          aten::is_same_size         0.02%       2.833us         0.02%       2.833us       0.086us           0 b           0 b            33  \n",
      "                               aten::type_as         0.00%       0.750us         0.16%      25.207us      25.207us           0 b           0 b             1  \n",
      "                                   aten::add         5.46%     864.491us         5.47%     865.492us      32.055us           0 b           0 b            27  \n",
      "                                   aten::mul        12.23%       1.936ms        12.75%       2.017ms      36.023us           0 b           0 b            56  \n",
      "--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 15.823ms\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 56\u001b[0m\n\u001b[1;32m     46\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(ProfilingCallback(device, n_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m))\n\u001b[1;32m     48\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     49\u001b[0m     model\u001b[38;5;241m=\u001b[39mpeft_model,\n\u001b[1;32m     50\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     53\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m     54\u001b[0m )\n\u001b[0;32m---> 56\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/scratch_repository/.venv/lib/python3.12/site-packages/transformers/trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/scratch_repository/.venv/lib/python3.12/site-packages/transformers/trainer.py:2393\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2387\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m   2388\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   2390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2391\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2392\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m-> 2393\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2394\u001b[0m ):\n\u001b[1;32m   2395\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2396\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2397\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    "import os\n",
    "from transformers import TrainingArguments\n",
    "import torch\n",
    "from transformers import Trainer\n",
    "\n",
    "device_type = \"cpu\"\n",
    "if torch.backends.mps.is_available():\n",
    "    device_type = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    device_type = \"cuda\"\n",
    "print(f\"Using device: {device_type}\")\n",
    "device = torch.device(device_type)\n",
    "peft_model.to(device)\n",
    "for param in peft_model.parameters():\n",
    "    param.data = param.data.to(device)\n",
    "\n",
    "if ENABLE_PROFILING:\n",
    "    # Print where each tensor is placed\n",
    "    for name, param in peft_model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f\"{name} is placed on {param.device}\")\n",
    "\n",
    "# Enable logging to wandb if WANDB_API_KEY is set\n",
    "wandb_enabled = os.getenv(\"WANDB_API_KEY\") is not None\n",
    "wandb_api_key = os.getenv(\"WANDB_API_KEY\", None)\n",
    "os.environ[\"WANDB_PROJECT\"] = \"hackathon\"\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"end\"\n",
    "os.environ[\"WANDB_WATCH\"] = \"false\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"../logs/training_{date.today().strftime('%Y-%m-%d')}\",\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=0.1,\n",
    "    label_names=[\"labels\"],\n",
    "    report_to=\"wandb\" if wandb_enabled else \"none\",\n",
    "    use_mps_device=(device_type == \"mps\"),\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    ")\n",
    "\n",
    "callbacks = []\n",
    "if ENABLE_PROFILING:\n",
    "    callbacks.append(ProfilingCallback(device, n_steps=10))\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "01jClKYXdZ-Z"
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0FDkmqV7dXNf"
   },
   "outputs": [],
   "source": [
    "def prepare_inference_input(smiles, precursor_mz):\n",
    "    inputs = tokenizer(\n",
    "        smiles,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    inputs[\"supplementary_data\"] = torch.tensor(\n",
    "        [supplementary_data], dtype=torch.float\n",
    "    ).to(device)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "peft_model.eval()\n",
    "\n",
    "# Example data\n",
    "smiles_example = \"CCO\"\n",
    "supplementary_data_example = 0  # TODO\n",
    "\n",
    "# Prepare input\n",
    "inputs = prepare_inference_input(smiles_example, supplementary_data_example)\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    outputs = peft_model(**inputs)\n",
    "    logits = outputs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2D8bfVV9keeM"
   },
   "source": [
    "# Choices that affect the whole architecture\n",
    "\n",
    "*   Format for the supplementary data\n",
    "*   Format for the label data\n",
    "*   The format of the output of the new model\n",
    "\n",
    "\n",
    "\n",
    "### More modular choices (that are important)\n",
    "\n",
    "\n",
    "*   Whether we have to predict compound_class at inference\n",
    "*   Include in_silico data?\n",
    "*   Architeture of the modified ChemBERTa model\n",
    "*   LoRA parameters\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
