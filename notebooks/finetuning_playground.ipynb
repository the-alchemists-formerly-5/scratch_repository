{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "DATASET = os.getenv(\"DATASET\", \"../data/raw/enveda_library_subset_10percent.parquet\")\n",
    "BASE_MODEL = \"seyonec/ChemBERTa-zinc-base-v1\"\n",
    "MAX_FRAGMENTS = 512 # from anton, max number of mzs/intensities\n",
    "MAX_SEQ_LENGTH = 512 # base model max seq length\n",
    "SUPPLEMENTARY_DATA_DIM = 75\n",
    "ENABLE_PROFILING = False # If turned on, will profile the training\n",
    "BATCH_SIZE = int(os.getenv(\"BATCH_SIZE\", 32)) # Note: if using CUDA, it'll automatically find the optimal batch size\n",
    "NUM_EPOCHS = int(os.getenv(\"NUM_EPOCHS\", 3))\n",
    "## Set WANDB_API_KEY environment variable to enable logging to wandb\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tYhe3fk-sblS"
   },
   "source": [
    "# Messing around with ChemBERTa for fun and for education\n",
    "\n",
    "The first half of this colab is just fun experiments trying to understand ChemBERTa and it's tokenizer better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 420,
     "referenced_widgets": [
      "09447f7d805a456a8d9df123079fef9b",
      "491c2c5348da4cebb59112f76ff21867",
      "fe9ba8c3b518443ba11fd1e700dd2c24",
      "017803aafcb84942bd04ae4b6014d713",
      "7bb64ec2bc31458fac09dafbafc66030",
      "48ba3adb18ee45b085dcfcc74b8e4d19",
      "90c266cbe71d46aaade3ab541889fe07",
      "ed10bed1b375495cb383786278b88953",
      "0186504a6df04889965a532aeaa3058c",
      "9365963655bf4a6f9f65418af1213afb",
      "e8e0cfd2025e41cfb5ea102586988b28",
      "6b54217022de4316a071ee63832b77fe",
      "3cf79467e3384aeba7e957596b912e29",
      "d23dd09659b9428f8cc8083f44e04709",
      "a75c8346a8f74f43941aa43700e6b2d8",
      "e5968c80140b4a71978dbb453ee75af2",
      "285f3c7db7d448a7a9ed62b2f9324e53",
      "e02702dc96744f0bb3304606cfa29e24",
      "5810f7fa342f4543834d8274abe552f5",
      "433bfc6fb35a4199b1cedfbaf5f5a15e",
      "8d37ac015a0f4caebf5455c7c16284dc",
      "f3084e63613741098216632a5f92ef98",
      "1b99d732290a4190982895ff640ea719",
      "b284f3ac0854497fbab925eeb1ded0df",
      "01638020753146678ee0a7692603b6c2",
      "6cd3e39a24fa4ed490ea0567c51c6eee",
      "e0c994de1d654e92ac91d661932ebdfa",
      "2fa221bf24cb4bdf9695b5f593b0d818",
      "502de19d537d41a7b151ca5798f46a72",
      "b689e92305ae4b6693991b37e894f4a0",
      "50e84fe3afd94297a3f198d0dfc4654b",
      "dcc379d6713f47529ea18e30482cfa29",
      "6d6719e9bdcc411c95f60201ee37119a",
      "ec2a4d26292d4606bdcee5c2a0ea5f70",
      "cf7c0c5e780145cf90de13cd375c1cce",
      "6cc566ec625c4251908f083a4ac5e972",
      "ea2e9aa05204487fa884178986a59380",
      "6ff00156a8d1407e8a03899aede36f60",
      "57196d402589425baba759c1f92b3fee",
      "931340e0216b4cdd940a7041f14b62f1",
      "ccba3a5e8f3f4f08b79e5cad740b78b8",
      "f762dd67830c45a1b8725313b92346e4",
      "bc455b59103e4404b9f9b124d6999b3c",
      "d178a1d70c32435482c10d6824bf4131",
      "7de2b548546d4c58b4e1af83cdf618e7",
      "b5c94d24b105493796b8f70d4adb55a7",
      "be8b9e625b624ad89abb785ffdb352b9",
      "7f60ccd803d8490f990a58518b1aa4dd",
      "038ff99e2ffc4f7db7ecd54333a3d71e",
      "177f0b132f2f40b9b63a048635f0dc70",
      "47da5e41d88d4bc6bfb496adacddba30",
      "604c8bed2d6f4c04bb9ae2b78c6f4aea",
      "c83d02b54fe94d6889aae124641f43eb",
      "4df1d8407ea74a3193426b27a15eeaa9",
      "67e3bc2be1034a73be1fc0b3b97da027",
      "f53b7a64750849d18bb4df54237a1110",
      "f7b208a676664b59a90d7d38293df4dd",
      "504f51d204af44b7915060072ea359b2",
      "1f2793e435b04dc7b9502f643a98d533",
      "c9e22b771e9e49ad8354902723d9b951",
      "201c3ab1ff3a46128961e12e4edb2a01",
      "bd46618a5bce46ec840bae0adb001564",
      "ac00b65c79fc4b73a3e2d9d437001e3f",
      "d9c52550dcb8430a891ada649a35d5de",
      "4a99e037f63b4dbfa9d2f58912c46167",
      "f89668c6b88c4bad853403c95eed4fff"
     ]
    },
    "id": "KPjZH305wYrn",
    "outputId": "a36beb4d-b472-44c7-aed8-c200e1cc515c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at seyonec/ChemBERTa-zinc-base-v1 were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import (AutoModelForMaskedLM, AutoTokenizer)\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(BASE_MODEL)\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_cMXHTFeMOy"
   },
   "source": [
    "# Data prep\n",
    "\n",
    "\n",
    "These are the columns in the data set:\n",
    "\n",
    "precursor_mz - f64\n",
    "precursor_charge - f64\n",
    "mzs - list[f64]\n",
    "intensities - list[f64]\n",
    "in_silico - bool\n",
    "smiles - str\n",
    "adduct - str\n",
    "collision_energy - str\n",
    "instrument_type - str\n",
    "compound_class - str\n",
    "entropy - f64\n",
    "scaffold_smiles - str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "nyTWxIYveLsl",
    "outputId": "b83c7ecc-b236-47ff-9062-4dcb1e3db1f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['precursor_mz', 'precursor_charge', 'mzs', 'intensities', 'in_silico', 'smiles', 'adduct', 'collision_energy', 'instrument_type', 'compound_class', 'entropy', 'scaffold_smiles']\n",
      "['precursor_mz', 'precursor_charge', 'mzs', 'intensities', 'in_silico', 'smiles', 'adduct', 'collision_energy', 'instrument_type', 'compound_class', 'entropy', 'scaffold_smiles']\n",
      "shape: (5, 12)\n",
      "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬──────────┬───────────┐\n",
      "│ precursor ┆ precursor ┆ mzs       ┆ intensiti ┆ … ┆ instrumen ┆ compound_ ┆ entropy  ┆ scaffold_ │\n",
      "│ _mz       ┆ _charge   ┆ ---       ┆ es        ┆   ┆ t_type    ┆ class     ┆ ---      ┆ smiles    │\n",
      "│ ---       ┆ ---       ┆ list[f64] ┆ ---       ┆   ┆ ---       ┆ ---       ┆ f64      ┆ ---       │\n",
      "│ f64       ┆ f64       ┆           ┆ list[f64] ┆   ┆ str       ┆ str       ┆          ┆ str       │\n",
      "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪══════════╪═══════════╡\n",
      "│ 657.54634 ┆ -1.0      ┆ [41.00329 ┆ [0.002206 ┆ … ┆ cfm-predi ┆ Triacylgl ┆ 4.160459 ┆           │\n",
      "│ 9         ┆           ┆ ,         ┆ ,         ┆   ┆ ct 4      ┆ ycerols   ┆          ┆           │\n",
      "│           ┆           ┆ 43.01894, ┆ 0.14623,  ┆   ┆           ┆           ┆          ┆           │\n",
      "│           ┆           ┆ …         ┆ …         ┆   ┆           ┆           ┆          ┆           │\n",
      "│           ┆           ┆ 657.546…  ┆ 0.588434… ┆   ┆           ┆           ┆          ┆           │\n",
      "│ 827.71231 ┆ 1.0       ┆ [43.05423 ┆ [0.122531 ┆ … ┆ cfm-predi ┆ Triacylgl ┆ 4.856278 ┆           │\n",
      "│ 7         ┆           ┆ ,         ┆ ,         ┆   ┆ ct 4      ┆ ycerols   ┆          ┆           │\n",
      "│           ┆           ┆ 45.06988, ┆ 0.013169, ┆   ┆           ┆           ┆          ┆           │\n",
      "│           ┆           ┆ …         ┆ …         ┆   ┆           ┆           ┆          ┆           │\n",
      "│           ┆           ┆ 827.712…  ┆ 0.89753…  ┆   ┆           ┆           ┆          ┆           │\n",
      "│ 949.72796 ┆ 1.0       ┆ [39.02293 ┆ [0.012077 ┆ … ┆ cfm-predi ┆ Triacylgl ┆ 5.097924 ┆           │\n",
      "│ 7         ┆           ┆ ,         ┆ ,         ┆   ┆ ct 4      ┆ ycerols   ┆          ┆           │\n",
      "│           ┆           ┆ 41.03858, ┆ 0.064455, ┆   ┆           ┆           ┆          ┆           │\n",
      "│           ┆           ┆ …         ┆ …         ┆   ┆           ┆           ┆          ┆           │\n",
      "│           ┆           ┆ 949.727…  ┆ 0.86479…  ┆   ┆           ┆           ┆          ┆           │\n",
      "│ 925.85935 ┆ -1.0      ┆ [44.9982, ┆ [0.027488 ┆ … ┆ cfm-predi ┆ Triacylgl ┆ 4.282513 ┆           │\n",
      "│ 1         ┆           ┆ 47.01385, ┆ ,         ┆   ┆ ct 4      ┆ ycerols   ┆          ┆           │\n",
      "│           ┆           ┆ …         ┆ 0.006652, ┆   ┆           ┆           ┆          ┆           │\n",
      "│           ┆           ┆ 925.8593… ┆ …         ┆   ┆           ┆           ┆          ┆           │\n",
      "│           ┆           ┆           ┆ 0.60526…  ┆   ┆           ┆           ┆          ┆           │\n",
      "│ 357.30103 ┆ -1.0      ┆ [41.00329 ┆ [0.072312 ┆ … ┆ cfm-predi ┆ Monoacylg ┆ 3.550469 ┆           │\n",
      "│ 4         ┆           ┆ ,         ┆ ,         ┆   ┆ ct 4      ┆ lycerols  ┆          ┆           │\n",
      "│           ┆           ┆ 43.01894, ┆ 0.146965, ┆   ┆           ┆           ┆          ┆           │\n",
      "│           ┆           ┆ …         ┆ …         ┆   ┆           ┆           ┆          ┆           │\n",
      "│           ┆           ┆ 357.301…  ┆ 0.25859…  ┆   ┆           ┆           ┆          ┆           │\n",
      "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴──────────┴───────────┘\n",
      "shape: (5, 12)\n",
      "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬──────────┬───────────┐\n",
      "│ precursor ┆ precursor ┆ mzs       ┆ intensiti ┆ … ┆ instrumen ┆ compound_ ┆ entropy  ┆ scaffold_ │\n",
      "│ _mz       ┆ _charge   ┆ ---       ┆ es        ┆   ┆ t_type    ┆ class     ┆ ---      ┆ smiles    │\n",
      "│ ---       ┆ ---       ┆ list[f64] ┆ ---       ┆   ┆ ---       ┆ ---       ┆ f64      ┆ ---       │\n",
      "│ f64       ┆ f64       ┆           ┆ list[f64] ┆   ┆ str       ┆ str       ┆          ┆ str       │\n",
      "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪══════════╪═══════════╡\n",
      "│ 527.13365 ┆ 1.0       ┆ [41.00219 ┆ [0.010587 ┆ … ┆ cfm-predi ┆ Chalcones ┆ 3.58425  ┆ O=C(CC(c1 │\n",
      "│ 8         ┆           ┆ ,         ┆ ,         ┆   ┆ ct 4      ┆           ┆          ┆ ccccc1)c1 │\n",
      "│           ┆           ┆ 43.01784, ┆ 0.023416, ┆   ┆           ┆           ┆          ┆ c(-c2cccc │\n",
      "│           ┆           ┆ …         ┆ …         ┆   ┆           ┆           ┆          ┆ c2)…      │\n",
      "│           ┆           ┆ 527.133…  ┆ 0.87760…  ┆   ┆           ┆           ┆          ┆           │\n",
      "│ 866.46180 ┆ 1.0       ┆ [41.03858 ┆ [0.15955, ┆ … ┆ cfm-predi ┆ Cyclic    ┆ 4.035891 ┆ O=C(CC12C │\n",
      "│ 9         ┆           ┆ ,         ┆ 0.176604, ┆   ┆ ct 4      ┆ peptides  ┆          ┆ CC(CO1)CO │\n",
      "│           ┆           ┆ 42.03383, ┆ … 1.0]    ┆   ┆           ┆           ┆          ┆ 2)NC1COC( │\n",
      "│           ┆           ┆ …         ┆           ┆   ┆           ┆           ┆          ┆ =O)…      │\n",
      "│           ┆           ┆ 866.461…  ┆           ┆   ┆           ┆           ┆          ┆           │\n",
      "│ 325.04844 ┆ -1.0      ┆ [34.9694, ┆ [0.335189 ┆ … ┆ cfm-predi ┆ Meroterpe ┆ 4.01902  ┆ O=C(CC12C │\n",
      "│           ┆           ┆ 41.00329, ┆ ,         ┆   ┆ ct 4      ┆ noids     ┆          ┆ CCC1COC2= │\n",
      "│           ┆           ┆ …         ┆ 0.514464, ┆   ┆           ┆ with      ┆          ┆ O)c1ccccc │\n",
      "│           ┆           ┆ 325.0484… ┆ … 1.0]    ┆   ┆           ┆ bridged   ┆          ┆ 1         │\n",
      "│           ┆           ┆           ┆           ┆   ┆           ┆ ri…       ┆          ┆           │\n",
      "│ 583.33777 ┆ 1.0       ┆ [30.03383 ┆ [0.009245 ┆ … ┆ cfm-predi ┆ Terpenoid ┆ 4.019998 ┆ O=C(CC12C │\n",
      "│ 8         ┆           ┆ ,         ┆ ,         ┆   ┆ ct 4      ┆ alkaloids ┆          ┆ CCC34C5CC │\n",
      "│           ┆           ┆ 32.04948, ┆ 0.007699, ┆   ┆           ┆           ┆          ┆ 6CCC(C5C6 │\n",
      "│           ┆           ┆ …         ┆ … 1.0]    ┆   ┆           ┆           ┆          ┆ )C(…      │\n",
      "│           ┆           ┆ 583.337…  ┆           ┆   ┆           ┆           ┆          ┆           │\n",
      "│ 679.40628 ┆ -1.0      ┆ [41.00329 ┆ [0.074812 ┆ … ┆ cfm-predi ┆ Ursane    ┆ 3.841568 ┆ O=C(CC12C │\n",
      "│ 7         ┆           ┆ ,         ┆ ,         ┆   ┆ ct 4      ┆ and Tarax ┆          ┆ CCCC1C1=C │\n",
      "│           ┆           ┆ 43.01894, ┆ 0.090091, ┆   ┆           ┆ astane    ┆          ┆ CC3C4CCCC │\n",
      "│           ┆           ┆ …         ┆ … 1.0]    ┆   ┆           ┆ triterp…  ┆          ┆ C4C…      │\n",
      "│           ┆           ┆ 679.406…  ┆           ┆   ┆           ┆           ┆          ┆           │\n",
      "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴──────────┴───────────┘\n",
      "Prepared data already exists for train split, skipping\n",
      "Reading prepared data from disk (prepared_train.parquet)\n",
      "Prepared data already exists for test split, skipping\n",
      "Reading prepared data from disk (prepared_test.parquet)\n"
     ]
    }
   ],
   "source": [
    "# import the data (with pandas?)\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "## Load the dataset (for some reason this didn't work for me)\n",
    "# df = pd.read_parquet('enveda_library_subset 2.parquet')\n",
    "\n",
    "# print(df.head())\n",
    "\n",
    "\n",
    "# custom Dataset class for all the types of data.\n",
    "# I think we might want to make a new 'column' of data that combines mzs and intensities into \"label\"\n",
    "\n",
    "from src.team5.data.data_loader import SMILESDataset\n",
    "from src.team5.data.data_split import sort_dataframe_by_scaffold, split_dataframe\n",
    "from src.team5.data.prepare import tensorize\n",
    "\n",
    "df = pl.read_parquet(DATASET)\n",
    "\n",
    "df_sorted = sort_dataframe_by_scaffold(df)\n",
    "\n",
    "df_train, df_test = split_dataframe(df_sorted, split_ratio=0.9)\n",
    "\n",
    "# Print column names\n",
    "print(df_train.columns)\n",
    "print(df_test.columns)\n",
    "print(df_train.head())\n",
    "print(df_test.head())\n",
    "\n",
    "(train_tokenized_smiles, train_attention_mask, train_labels, train_supplementary_data) = tensorize(df_train, split=\"train\")\n",
    "(test_tokenized_smiles, test_attention_mask, test_labels, test_supplementary_data) = tensorize(df_test, split=\"test\")\n",
    "\n",
    "train_dataset = SMILESDataset(train_tokenized_smiles, train_attention_mask, train_labels, train_supplementary_data)\n",
    "test_dataset = SMILESDataset(test_tokenized_smiles, test_attention_mask, test_labels, test_supplementary_data)\n",
    "\n",
    "## batch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': torch.int64, 'attention_mask': torch.int64, 'labels': torch.float32, 'supplementary_data': torch.float32}\n"
     ]
    }
   ],
   "source": [
    "print({k:v.dtype for k,v in train_dataset[0].items()})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fENovYIHeaOO"
   },
   "source": [
    "# Custom model for our problem\n",
    "This is probably the most important part in terms of design choices. We are changing the ChemBERTa model by adding on something at the end. This new module will take the hidden SMILES embedding from the last hidden layer as input. It will also take in all the other data about the precusor molecule and experimental conditions (eg, precusor mz, collison energy etc). For now, let's call that supplementary data.\n",
    "\n",
    "I've written the simplest possible thing here: a single linear layer that takes the embedding of the entire seq, concatinated with all the supplementary data for the example. It outputs \"labels\", which is mzs and intensities zipped together.\n",
    "\n",
    "The reason for making a single module output both mzs and intensities is because there needs to be the same number of fragments per example, and the two numbers are very related.\n",
    "\n",
    "A single linear layer is probably a terrible choice though, since this is the only layer that sees all the supplementary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "rMogXqZteZ1Z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomChemBERTaModel(\n",
      "  (model): RobertaForMaskedLM(\n",
      "    (roberta): RobertaModel(\n",
      "      (embeddings): RobertaEmbeddings(\n",
      "        (word_embeddings): Embedding(767, 768, padding_idx=1)\n",
      "        (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "        (token_type_embeddings): Embedding(1, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): RobertaEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0-5): 6 x RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSdpaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (lm_head): RobertaLMHead(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (decoder): Linear(in_features=768, out_features=767, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (final_layers): FinalLayers(\n",
      "    (layer1): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (activation1): ReLU()\n",
      "    (dropout1): Dropout(p=0.1, inplace=False)\n",
      "    (layernorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (layer2): Linear(in_features=843, out_features=2, bias=True)\n",
      "    (activation2): ReLU()\n",
      "    (dropout2): Dropout(p=0.1, inplace=False)\n",
      "    (layernorm2): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n",
      "model.roberta.embeddings.word_embeddings.weight has shape torch.Size([767, 768])\n",
      "model.roberta.embeddings.position_embeddings.weight has shape torch.Size([514, 768])\n",
      "model.roberta.embeddings.token_type_embeddings.weight has shape torch.Size([1, 768])\n",
      "model.roberta.embeddings.LayerNorm.weight has shape torch.Size([768])\n",
      "model.roberta.embeddings.LayerNorm.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.0.attention.self.query.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.0.attention.self.query.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.0.attention.self.key.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.0.attention.self.key.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.0.attention.self.value.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.0.attention.self.value.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.0.attention.output.dense.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.0.attention.output.dense.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.0.attention.output.LayerNorm.weight has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.0.attention.output.LayerNorm.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.0.intermediate.dense.weight has shape torch.Size([3072, 768])\n",
      "model.roberta.encoder.layer.0.intermediate.dense.bias has shape torch.Size([3072])\n",
      "model.roberta.encoder.layer.0.output.dense.weight has shape torch.Size([768, 3072])\n",
      "model.roberta.encoder.layer.0.output.dense.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.0.output.LayerNorm.weight has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.0.output.LayerNorm.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.1.attention.self.query.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.1.attention.self.query.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.1.attention.self.key.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.1.attention.self.key.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.1.attention.self.value.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.1.attention.self.value.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.1.attention.output.dense.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.1.attention.output.dense.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.1.attention.output.LayerNorm.weight has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.1.attention.output.LayerNorm.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.1.intermediate.dense.weight has shape torch.Size([3072, 768])\n",
      "model.roberta.encoder.layer.1.intermediate.dense.bias has shape torch.Size([3072])\n",
      "model.roberta.encoder.layer.1.output.dense.weight has shape torch.Size([768, 3072])\n",
      "model.roberta.encoder.layer.1.output.dense.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.1.output.LayerNorm.weight has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.1.output.LayerNorm.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.2.attention.self.query.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.2.attention.self.query.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.2.attention.self.key.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.2.attention.self.key.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.2.attention.self.value.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.2.attention.self.value.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.2.attention.output.dense.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.2.attention.output.dense.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.2.attention.output.LayerNorm.weight has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.2.attention.output.LayerNorm.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.2.intermediate.dense.weight has shape torch.Size([3072, 768])\n",
      "model.roberta.encoder.layer.2.intermediate.dense.bias has shape torch.Size([3072])\n",
      "model.roberta.encoder.layer.2.output.dense.weight has shape torch.Size([768, 3072])\n",
      "model.roberta.encoder.layer.2.output.dense.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.2.output.LayerNorm.weight has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.2.output.LayerNorm.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.3.attention.self.query.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.3.attention.self.query.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.3.attention.self.key.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.3.attention.self.key.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.3.attention.self.value.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.3.attention.self.value.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.3.attention.output.dense.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.3.attention.output.dense.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.3.attention.output.LayerNorm.weight has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.3.attention.output.LayerNorm.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.3.intermediate.dense.weight has shape torch.Size([3072, 768])\n",
      "model.roberta.encoder.layer.3.intermediate.dense.bias has shape torch.Size([3072])\n",
      "model.roberta.encoder.layer.3.output.dense.weight has shape torch.Size([768, 3072])\n",
      "model.roberta.encoder.layer.3.output.dense.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.3.output.LayerNorm.weight has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.3.output.LayerNorm.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.4.attention.self.query.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.4.attention.self.query.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.4.attention.self.key.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.4.attention.self.key.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.4.attention.self.value.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.4.attention.self.value.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.4.attention.output.dense.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.4.attention.output.dense.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.4.attention.output.LayerNorm.weight has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.4.attention.output.LayerNorm.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.4.intermediate.dense.weight has shape torch.Size([3072, 768])\n",
      "model.roberta.encoder.layer.4.intermediate.dense.bias has shape torch.Size([3072])\n",
      "model.roberta.encoder.layer.4.output.dense.weight has shape torch.Size([768, 3072])\n",
      "model.roberta.encoder.layer.4.output.dense.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.4.output.LayerNorm.weight has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.4.output.LayerNorm.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.5.attention.self.query.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.5.attention.self.query.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.5.attention.self.key.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.5.attention.self.key.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.5.attention.self.value.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.5.attention.self.value.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.5.attention.output.dense.weight has shape torch.Size([768, 768])\n",
      "model.roberta.encoder.layer.5.attention.output.dense.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.5.attention.output.LayerNorm.weight has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.5.attention.output.LayerNorm.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.5.intermediate.dense.weight has shape torch.Size([3072, 768])\n",
      "model.roberta.encoder.layer.5.intermediate.dense.bias has shape torch.Size([3072])\n",
      "model.roberta.encoder.layer.5.output.dense.weight has shape torch.Size([768, 3072])\n",
      "model.roberta.encoder.layer.5.output.dense.bias has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.5.output.LayerNorm.weight has shape torch.Size([768])\n",
      "model.roberta.encoder.layer.5.output.LayerNorm.bias has shape torch.Size([768])\n",
      "model.lm_head.bias has shape torch.Size([767])\n",
      "model.lm_head.dense.weight has shape torch.Size([768, 768])\n",
      "model.lm_head.dense.bias has shape torch.Size([768])\n",
      "model.lm_head.layer_norm.weight has shape torch.Size([768])\n",
      "model.lm_head.layer_norm.bias has shape torch.Size([768])\n",
      "final_layers.layer1.weight has shape torch.Size([512, 512])\n",
      "final_layers.layer1.bias has shape torch.Size([512])\n",
      "final_layers.layernorm1.weight has shape torch.Size([512])\n",
      "final_layers.layernorm1.bias has shape torch.Size([512])\n",
      "final_layers.layer2.weight has shape torch.Size([2, 843])\n",
      "final_layers.layer2.bias has shape torch.Size([2])\n",
      "final_layers.layernorm2.weight has shape torch.Size([2])\n",
      "final_layers.layernorm2.bias has shape torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "from src.team5.models.custom_model import CustomChemBERTaModel\n",
    "\n",
    "MS_model = CustomChemBERTaModel(model, MAX_FRAGMENTS, MAX_SEQ_LENGTH, SUPPLEMENTARY_DATA_DIM)\n",
    "\n",
    "print(MS_model)\n",
    "\n",
    "for name, param in MS_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name} has shape {param.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IDwMMYCaYAZK"
   },
   "source": [
    "# LoRA config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "y0PF6-qGZ683"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 412,828 || all params: 44,784,439 || trainable%: 0.9218\n",
      "base_model.model.model.roberta.encoder.layer.0.attention.self.query.lora_A.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.0.attention.self.query.lora_B.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.0.attention.self.value.lora_A.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.0.attention.self.value.lora_B.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.1.attention.self.query.lora_A.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.1.attention.self.query.lora_B.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.1.attention.self.value.lora_A.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.1.attention.self.value.lora_B.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.2.attention.self.query.lora_A.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.2.attention.self.query.lora_B.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.2.attention.self.value.lora_A.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.2.attention.self.value.lora_B.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.3.attention.self.query.lora_A.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.3.attention.self.query.lora_B.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.3.attention.self.value.lora_A.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.3.attention.self.value.lora_B.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.4.attention.self.query.lora_A.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.4.attention.self.query.lora_B.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.4.attention.self.value.lora_A.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.4.attention.self.value.lora_B.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.5.attention.self.query.lora_A.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.5.attention.self.query.lora_B.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.5.attention.self.value.lora_A.default.weight is trainable\n",
      "base_model.model.model.roberta.encoder.layer.5.attention.self.value.lora_B.default.weight is trainable\n",
      "base_model.model.final_layers.modules_to_save.default.layer1.weight is trainable\n",
      "base_model.model.final_layers.modules_to_save.default.layer1.bias is trainable\n",
      "base_model.model.final_layers.modules_to_save.default.layernorm1.weight is trainable\n",
      "base_model.model.final_layers.modules_to_save.default.layernorm1.bias is trainable\n",
      "base_model.model.final_layers.modules_to_save.default.layer2.weight is trainable\n",
      "base_model.model.final_layers.modules_to_save.default.layer2.bias is trainable\n",
      "base_model.model.final_layers.modules_to_save.default.layernorm2.weight is trainable\n",
      "base_model.model.final_layers.modules_to_save.default.layernorm2.bias is trainable\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"query\", \"value\"],\n",
    "    modules_to_save=[\n",
    "        \"final_layers\"\n",
    "    ],  # change this to the name of the new modules at the end.\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(MS_model, peft_config)\n",
    "\n",
    "peft_model.print_trainable_parameters()  # check that it's training the right things\n",
    "for name, param in peft_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name} is trainable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-q-zpo0kaF0p"
   },
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import torch.nn as nn\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "class ProfilingCallback(TrainerCallback):\n",
    "    def __init__(self, device, n_steps=10):\n",
    "        self.device = device\n",
    "        self.n_steps = n_steps\n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if state.global_step % self.n_steps == 0:\n",
    "            with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "                         profile_memory=True, record_shapes=True) as prof:\n",
    "                with record_function(\"model_inference\"):\n",
    "                    # Run a forward pass\n",
    "                    example = train_dataset[0]\n",
    "                    # Each field in the example is a tensor, so we need to add a batch dimension to the front of each\n",
    "                    example = {k: v.unsqueeze(0).to(self.device) for k, v in example.items()}\n",
    "                    peft_model(**example)\n",
    "            \n",
    "            print(f\"Step {state.global_step}\")\n",
    "            print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=20))\n",
    "            print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=20))\n",
    "            print(prof.key_averages().table(sort_by=\"cpu_memory_usage\", row_limit=20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lars/workspace/scratch_repository/.venv/lib/python3.12/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lars/workspace/scratch_repository/.venv/lib/python3.12/site-packages/transformers/training_args.py:2199: UserWarning: `use_mps_device` is deprecated and will be removed in version 5.0 of 🤗 Transformers. `mps` device will be used by default if available similar to the way `cuda` device is used.Therefore, no action from user is required. \n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 93,474\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 8,766\n",
      "  Number of trainable parameters = 412,828\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4ce46502e624a769032be3e7477688f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8766 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 60\u001b[0m\n\u001b[1;32m     50\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(ProfilingCallback(device, n_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m))\n\u001b[1;32m     52\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     53\u001b[0m     model\u001b[38;5;241m=\u001b[39mpeft_model,\n\u001b[1;32m     54\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m     58\u001b[0m )\n\u001b[0;32m---> 60\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/scratch_repository/.venv/lib/python3.12/site-packages/transformers/trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/scratch_repository/.venv/lib/python3.12/site-packages/transformers/trainer.py:2388\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2387\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2388\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2391\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2392\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2393\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2394\u001b[0m ):\n\u001b[1;32m   2395\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2396\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/workspace/scratch_repository/.venv/lib/python3.12/site-packages/transformers/trainer.py:3518\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3516\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3518\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3520\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m~/workspace/scratch_repository/.venv/lib/python3.12/site-packages/accelerate/accelerator.py:2246\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2244\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2245\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2246\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/scratch_repository/.venv/lib/python3.12/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/scratch_repository/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/scratch_repository/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from datetime import date, datetime\n",
    "import os\n",
    "from transformers import TrainingArguments\n",
    "import torch\n",
    "from transformers import Trainer\n",
    "import transformers\n",
    "\n",
    "transformers.logging.set_verbosity_info()\n",
    "\n",
    "device_type = \"cpu\"\n",
    "if torch.backends.mps.is_available():\n",
    "    device_type = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    device_type = \"cuda\"\n",
    "print(f\"Using device: {device_type}\")\n",
    "device = torch.device(device_type)\n",
    "peft_model.to(device)\n",
    "for param in peft_model.parameters():\n",
    "    param.data = param.data.to(device)\n",
    "\n",
    "if ENABLE_PROFILING:\n",
    "    # Print where each tensor is placed\n",
    "    for name, param in peft_model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f\"{name} is placed on {param.device}\")\n",
    "\n",
    "# Enable logging to wandb if WANDB_API_KEY is set\n",
    "wandb_enabled = os.getenv(\"WANDB_API_KEY\") is not None\n",
    "wandb_api_key = os.getenv(\"WANDB_API_KEY\", None)\n",
    "os.environ[\"WANDB_PROJECT\"] = \"hackathon\"\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"end\"\n",
    "os.environ[\"WANDB_WATCH\"] = \"false\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"../logs/training_{date.today().strftime('%Y-%m-%d')}-{datetime.now().strftime('%H-%M-%S')}\",\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    dataloader_num_workers=8,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_steps=0.01,\n",
    "    eval_steps=0.05,\n",
    "    label_names=[\"labels\"],\n",
    "    report_to=\"wandb\" if wandb_enabled else \"none\",\n",
    "    auto_find_batch_size=(device_type == \"cuda\"),\n",
    "    use_mps_device=(device_type == \"mps\"),\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    ")\n",
    "\n",
    "callbacks = []\n",
    "if ENABLE_PROFILING:\n",
    "    callbacks.append(ProfilingCallback(device, n_steps=10))\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2D8bfVV9keeM"
   },
   "source": [
    "# Choices that affect the whole architecture\n",
    "\n",
    "*   Format for the supplementary data\n",
    "*   Format for the label data\n",
    "*   The format of the output of the new model\n",
    "\n",
    "\n",
    "\n",
    "### More modular choices (that are important)\n",
    "\n",
    "\n",
    "*   Whether we have to predict compound_class at inference\n",
    "*   Include in_silico data?\n",
    "*   Architeture of the modified ChemBERTa model\n",
    "*   LoRA parameters\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
